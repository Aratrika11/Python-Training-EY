{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Struct Type Practice\").getOrCreate()"
      ],
      "metadata": {
        "id": "52eMmb0ApU8v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZKKLDIv0hm4d"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType, LongType)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = [\n",
        "    (\"U001\",\"Abhishek\",28,\"Hyderabad\",50000),\n",
        "    (\"U002\",\"Neha\",32,\"Delhi\",62000),\n",
        "    (\"U003\",\"Ravi\",25,\"Bangalore\",45000),\n",
        "    (\"U004\",\"Pooja\",29,\"Mumbai\",58000)\n",
        "]\n",
        "\n",
        "user_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), nullable=False),\n",
        "    StructField(\"name\", StringType(), nullable=True),\n",
        "    StructField(\"age\", IntegerType(), nullable=True),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"salary\", IntegerType(), nullable=True),\n",
        "])\n",
        "\n",
        "df_users = spark.createDataFrame(raw_data, schema = user_schema)\n",
        "df_users.printSchema()\n",
        "df_users.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyITvcvOpg8d",
        "outputId": "2f051eef-3622-435d-d112-a1a53f61ebdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+-------+--------+---+---------+------+\n",
            "|user_id|    name|age|     city|salary|\n",
            "+-------+--------+---+---------+------+\n",
            "|   U001|Abhishek| 28|Hyderabad| 50000|\n",
            "|   U002|    Neha| 32|    Delhi| 62000|\n",
            "|   U003|    Ravi| 25|Bangalore| 45000|\n",
            "|   U004|   Pooja| 29|   Mumbai| 58000|\n",
            "+-------+--------+---+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrong Data\n",
        "raw_data2 = [(\n",
        "    \"U005\",\"Amit\",\"Thirty\",\"Chennai\",40000\n",
        ")]\n",
        "df_users = spark.createDataFrame(raw_data2, schema = user_schema)\n",
        "df_users.show()"
      ],
      "metadata": {
        "id": "SbUJgln4qVN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import ArrayType\n",
        "\n",
        "interest_data = [\n",
        "    (\"U001\",[\"AI\",\"ML\",\"Cloud\"]),\n",
        "    (\"U002\",[\"Testing\",\"Automation\"]),\n",
        "    (\"U003\",[\"Data Engineering\",\"Spark\",\"Kafka\"]),\n",
        "    (\"U004\",[\"UI/UX\"])\n",
        "]"
      ],
      "metadata": {
        "id": "zcMjdQm6rK9z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interest_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(),False),\n",
        "    StructField(\"interests\", ArrayType(StringType()), nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "CA08XX6krZPE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_interests = spark.createDataFrame(interest_data, schema=interest_schema)\n",
        "df_interests.printSchema()\n",
        "df_interests.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK5x-bjtri41",
        "outputId": "c68a399a-f876-4999-e0da-5eaad914483e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+-------+--------------------------------+\n",
            "|user_id|interests                       |\n",
            "+-------+--------------------------------+\n",
            "|U001   |[AI, ML, Cloud]                 |\n",
            "|U002   |[Testing, Automation]           |\n",
            "|U003   |[Data Engineering, Spark, Kafka]|\n",
            "|U004   |[UI/UX]                         |\n",
            "+-------+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df_interests = df_interests.select(\"user_id\", explode(\"interests\").alias(\"interest\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KY-OsCRsIC3",
        "outputId": "9158fe5d-5ebd-490c-98a4-b17b0ef73c95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+\n",
            "|user_id|        interest|\n",
            "+-------+----------------+\n",
            "|   U001|              AI|\n",
            "|   U001|              ML|\n",
            "|   U001|           Cloud|\n",
            "|   U002|         Testing|\n",
            "|   U002|      Automation|\n",
            "|   U003|Data Engineering|\n",
            "|   U003|           Spark|\n",
            "|   U003|           Kafka|\n",
            "|   U004|           UI/UX|\n",
            "+-------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Type\n",
        "from pyspark.sql.types import MapType"
      ],
      "metadata": {
        "id": "onxjJwchsuXy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_data = [\n",
        "    (\"U001\",{\"mobile\":120,\"laptop\":300}),\n",
        "    (\"U002\",{\"tablet\":80}),\n",
        "    (\"U003\",{\"mobile\":200,\"desktop\":400}),\n",
        "    (\"U004\",{\"laptop\":250})\n",
        "]"
      ],
      "metadata": {
        "id": "wn-wJTGosy4b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"device_usuage\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "df_devices = spark.createDataFrame(device_data, schema=device_schema)\n",
        "df_devices.printSchema()\n",
        "df_devices.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhAAwrR5tEPn",
        "outputId": "ddb881ab-7c49-439d-e496-a6ec1393b6d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- device_usuage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n",
            "+-------+-------------------------------+\n",
            "|user_id|device_usuage                  |\n",
            "+-------+-------------------------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |\n",
            "|U002   |{tablet -> 80}                 |\n",
            "|U003   |{mobile -> 200, desktop -> 400}|\n",
            "|U004   |{laptop -> 250}                |\n",
            "+-------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nested_data = [\n",
        "    (\"U001\",(\"Hyderabad\",\"Telangana\",500081)),\n",
        "    (\"U002\",(\"Delhi\",\"Delhi\",110001)),\n",
        "    (\"U003\",(\"Bangalore\",\"Karnataka\",560001))\n",
        "]"
      ],
      "metadata": {
        "id": "yCgqWXAwuUX1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "address_schema = StructType([\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"state\", StringType(), False),\n",
        "    StructField(\"pincode\",IntegerType(), True)\n",
        "])\n",
        "\n",
        "profile_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"address\", address_schema, True)\n",
        "])"
      ],
      "metadata": {
        "id": "8euP0cCauEMi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_profiles = spark.createDataFrame(nested_data, schema=profile_schema)\n",
        "df_profiles.printSchema()\n",
        "df_profiles.show(truncate=False)\n",
        "#df_profiles.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miB7s7wSuvtZ",
        "outputId": "63efe3b2-4ba3-413b-95c7-6e8a4791abdb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = false)\n",
            " |    |-- pincode: integer (nullable = true)\n",
            "\n",
            "+-------+------------------------------+\n",
            "|user_id|address                       |\n",
            "+-------+------------------------------+\n",
            "|U001   |{Hyderabad, Telangana, 500081}|\n",
            "|U002   |{Delhi, Delhi, 110001}        |\n",
            "|U003   |{Bangalore, Karnataka, 560001}|\n",
            "+-------+------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_profiles.select(\n",
        "    \"user_id\",\n",
        "    \"address.city\",\n",
        "    \"address.state\",\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1Hkyx6ev9TY",
        "outputId": "c3f43e91-5ede-4f06-c683-9bb6a5abe132"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+\n",
            "|user_id|     city|    state|\n",
            "+-------+---------+---------+\n",
            "|   U001|Hyderabad|Telangana|\n",
            "|   U002|    Delhi|    Delhi|\n",
            "|   U003|Bangalore|Karnataka|\n",
            "+-------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_users.withColumn(\"salary_int\", col(\"salary\").cast(\"int\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPvb9WewwVGH",
        "outputId": "b9253986-af05-4f09-834b-8eaa54308840"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+---+---------+------+----------+\n",
            "|user_id|    name|age|     city|salary|salary_int|\n",
            "+-------+--------+---+---------+------+----------+\n",
            "|   U001|Abhishek| 28|Hyderabad| 50000|     50000|\n",
            "|   U002|    Neha| 32|    Delhi| 62000|     62000|\n",
            "|   U003|    Ravi| 25|Bangalore| 45000|     45000|\n",
            "|   U004|   Pooja| 29|   Mumbai| 58000|     58000|\n",
            "+-------+--------+---+---------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "df_orders.withColumn(\"order_date\", to_date(\"order_date\", \"yyyy-MM-dd\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "VuHlM3lxxR5x",
        "outputId": "f2a8df76-819d-4ff9-c76a-84e321a24812"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_orders' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2247886449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_orders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yyyy-MM-dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_orders' is not defined"
          ]
        }
      ]
    }
  ]
}