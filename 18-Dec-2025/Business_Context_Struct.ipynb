{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_xhLgLCg2rI-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType, LongType)\n",
        "spark = SparkSession.builder.appName(\"Digital Learning Platform\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 1 — USER REGISTRATION (CORRUPTED SCHEMA)"
      ],
      "metadata": {
        "id": "IE_0JLjTR0xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "\n",
        "1. Design an explicit schema using StructType\n",
        "2. Normalize age into IntegerType\n",
        "3. Normalize skills into ArrayType\n",
        "4. Handle empty or missing names\n",
        "5. Produce a clean users_df"
      ],
      "metadata": {
        "id": "-tm914VcSFkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "WXugjL0bR8Ha"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "raw_users = [\n",
        "(\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n",
        "(\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n",
        "(\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "(\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n",
        "(\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")]\n",
        "\n",
        "user_schema = StructType([\n",
        "StructField(\"user_id\", StringType(), True),\n",
        "StructField(\"name\", StringType(), True),\n",
        "StructField(\"age_raw\", StringType(), True),\n",
        "StructField(\"city\", StringType(), True),\n",
        "StructField(\"skills_raw\", StringType(), True)])\n",
        "df_raw = spark.createDataFrame(raw_users, user_schema)\n",
        "df_raw.printSchema()\n",
        "df_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNi4ccmHRwD1",
        "outputId": "63338704-b93d-4722-d99d-6d55297c67e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age_raw: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- skills_raw: string (nullable = true)\n",
            "\n",
            "+-------+-----+-------+---------+-------------------+\n",
            "|user_id| name|age_raw|     city|         skills_raw|\n",
            "+-------+-----+-------+---------+-------------------+\n",
            "|   U001| Amit|     28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha| Thirty|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|   NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|     29|   Mumbai|               NULL|\n",
            "|   U005|     |     31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+-------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "df_age = df_raw.withColumn(\"age\", when(col(\"age_raw\").rlike(\"^[0-9]+$\"),\n",
        "col(\"age_raw\").cast(IntegerType())).otherwise(None))\n",
        "df_age.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdmQ1oRgSpCu",
        "outputId": "8641d32a-acf7-481a-acfa-95bf3b0cc6ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-------+---------+-------------------+----+\n",
            "|user_id| name|age_raw|     city|         skills_raw| age|\n",
            "+-------+-----+-------+---------+-------------------+----+\n",
            "|   U001| Amit|     28|Hyderabad|['AI','ML','Cloud']|  28|\n",
            "|   U002| Neha| Thirty|    Delhi|         AI,Testing|NULL|\n",
            "|   U003| Ravi|   NULL|Bangalore|      [Data, Spark]|NULL|\n",
            "|   U004|Pooja|     29|   Mumbai|               NULL|  29|\n",
            "|   U005|     |     31|  Chennai|         ['DevOps']|  31|\n",
            "+-------+-----+-------+---------+-------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "df_skills = df_age.withColumn(\"skills\", when(col(\"skills_raw\").isNull(), array())\n",
        ".when(col(\"skills_raw\").startswith(\"[\"), split(regexp_replace(col(\"skills_raw\"), \"[\\[\\]']\", \"\"), \",\")).otherwise(split(col(\"skills_raw\"), \",\\s\")))\n",
        "df_skills.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQL3Ei90So1C",
        "outputId": "4f87c257-af0d-4a3b-f129-e2d08229fc34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\['\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\['\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-2163614263.py:3: SyntaxWarning: invalid escape sequence '\\['\n",
            "  .when(col(\"skills_raw\").startswith(\"[\"), split(regexp_replace(col(\"skills_raw\"), \"[\\[\\]']\", \"\"), \",\")).otherwise(split(col(\"skills_raw\"), \",\\s\")))\n",
            "/tmp/ipython-input-2163614263.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  .when(col(\"skills_raw\").startswith(\"[\"), split(regexp_replace(col(\"skills_raw\"), \"[\\[\\]']\", \"\"), \",\")).otherwise(split(col(\"skills_raw\"), \",\\s\")))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-------+---------+-------------------+----+---------------+\n",
            "|user_id| name|age_raw|     city|         skills_raw| age|         skills|\n",
            "+-------+-----+-------+---------+-------------------+----+---------------+\n",
            "|   U001| Amit|     28|Hyderabad|['AI','ML','Cloud']|  28|[AI, ML, Cloud]|\n",
            "|   U002| Neha| Thirty|    Delhi|         AI,Testing|NULL|   [AI,Testing]|\n",
            "|   U003| Ravi|   NULL|Bangalore|      [Data, Spark]|NULL| [Data,  Spark]|\n",
            "|   U004|Pooja|     29|   Mumbai|               NULL|  29|             []|\n",
            "|   U005|     |     31|  Chennai|         ['DevOps']|  31|       [DevOps]|\n",
            "+-------+-----+-------+---------+-------------------+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "df_name = df_skills.withColumn(\"name\", when(col(\"name\").isNull() | (trim(col(\"name\")) == \"\"), lit(\"Unknown\")).otherwise(col(\"name\")))\n",
        "df_name.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1Rdo6r7S5jI",
        "outputId": "16007bbf-db70-439a-d901-4b5f96951cfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+---------+-------------------+----+---------------+\n",
            "|user_id|   name|age_raw|     city|         skills_raw| age|         skills|\n",
            "+-------+-------+-------+---------+-------------------+----+---------------+\n",
            "|   U001|   Amit|     28|Hyderabad|['AI','ML','Cloud']|  28|[AI, ML, Cloud]|\n",
            "|   U002|   Neha| Thirty|    Delhi|         AI,Testing|NULL|   [AI,Testing]|\n",
            "|   U003|   Ravi|   NULL|Bangalore|      [Data, Spark]|NULL| [Data,  Spark]|\n",
            "|   U004|  Pooja|     29|   Mumbai|               NULL|  29|             []|\n",
            "|   U005|Unknown|     31|  Chennai|         ['DevOps']|  31|       [DevOps]|\n",
            "+-------+-------+-------+---------+-------------------+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "df_clean_users = df_name.select(\"user_id\", \"name\", \"age\", \"city\", \"skills\")\n",
        "df_clean_users.show(truncate=False)\n",
        "df_clean_users.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8rTdgY2TAC1",
        "outputId": "41804fe2-1be3-459d-8928-6cc276896990"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+---------------+\n",
            "|user_id|name   |age |city     |skills         |\n",
            "+-------+-------+----+---------+---------------+\n",
            "|U001   |Amit   |28  |Hyderabad|[AI, ML, Cloud]|\n",
            "|U002   |Neha   |NULL|Delhi    |[AI,Testing]   |\n",
            "|U003   |Ravi   |NULL|Bangalore|[Data,  Spark] |\n",
            "|U004   |Pooja  |29  |Mumbai   |[]             |\n",
            "|U005   |Unknown|31  |Chennai  |[DevOps]       |\n",
            "+-------+-------+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 2 — COURSE CATALOG (NESTED STRUCT)"
      ],
      "metadata": {
        "id": "ELmmxx89Tjvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "\n",
        "1. Create nested StructType for course metadata\n",
        "2. Normalize domain and level\n",
        "3. Convert price to IntegerType\n",
        "4. Handle missing prices\n",
        "5. Produce courses_df"
      ],
      "metadata": {
        "id": "DvF27EKcTpWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "raw_courses = [\n",
        "(\"C001\",\"PySpark Mastery\",(\"Data Engineering\",\"Advanced\"),\"₹9999\"),\n",
        "(\"C002\",\"AI for Testers\",{\"domain\":\"QA\",\"level\":\"Beginner\"},\"8999\"),\n",
        "(\"C003\",\"ML Foundations\",(\"AI\",\"Intermediate\"),None),\n",
        "(\"C004\",\"Data Engineering Bootcamp\",\"Data|Advanced\",\"₹14999\")\n",
        "]\n",
        "course_schema = StructType([\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"course_name\", StringType(), True),\n",
        "    StructField(\"metadata_raw\", StringType(), True),\n",
        "    StructField(\"price_raw\", StringType(), True)\n",
        "])\n",
        "df_courses_raw = spark.createDataFrame(raw_courses, course_schema)\n",
        "df_courses_raw.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJElDKHWTt3v",
        "outputId": "f0684eaf-fe5d-4552-d951-0ee82ef7df8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- metadata_raw: string (nullable = true)\n",
            " |-- price_raw: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "df_courses_normalized = df_courses_raw.withColumn(\"domain\", \\\n",
        "    when(col(\"metadata_raw\").startswith(\"(\"), regexp_extract(col(\"metadata_raw\"), \"\\('([^']+)',\\s*'([^']+)'\\)\", 1)) \\\n",
        "    .when(col(\"metadata_raw\").startswith(\"{\"), regexp_extract(col(\"metadata_raw\"), \"'domain':\\s*'([^']+)'\", 1)) \\\n",
        "    .when(col(\"metadata_raw\").contains(\"|\"), split(col(\"metadata_raw\"), \"\\|\").getItem(0)) \\\n",
        "    .otherwise(lit(None))) \\\n",
        ".withColumn(\"level\", \\\n",
        "    when(col(\"metadata_raw\").startswith(\"(\"), regexp_extract(col(\"metadata_raw\"), \"\\('([^']+)',\\s*'([^']+)'\\)\", 2)) \\\n",
        "    .when(col(\"metadata_raw\").startswith(\"{\"), regexp_extract(col(\"metadata_raw\"), \"'level':\\s*'([^']+)'\", 1)) \\\n",
        "    .when(col(\"metadata_raw\").contains(\"|\"), split(col(\"metadata_raw\"), \"\\|\").getItem(1)) \\\n",
        "    .otherwise(lit(None)))\n",
        "\n",
        "df_courses_normalized.show(truncate=False)\n",
        "df_courses_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ3JsWyNT5Y5",
        "outputId": "7babfb99-16e1-4eb1-e103-41559eb1a73c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\|'\n",
            "<>:8: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\|'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\|'\n",
            "<>:8: SyntaxWarning: invalid escape sequence '\\('\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\|'\n",
            "/tmp/ipython-input-927040372.py:3: SyntaxWarning: invalid escape sequence '\\('\n",
            "  when(col(\"metadata_raw\").startswith(\"(\"), regexp_extract(col(\"metadata_raw\"), \"\\('([^']+)',\\s*'([^']+)'\\)\", 1)) \\\n",
            "/tmp/ipython-input-927040372.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  .when(col(\"metadata_raw\").startswith(\"{\"), regexp_extract(col(\"metadata_raw\"), \"'domain':\\s*'([^']+)'\", 1)) \\\n",
            "/tmp/ipython-input-927040372.py:5: SyntaxWarning: invalid escape sequence '\\|'\n",
            "  .when(col(\"metadata_raw\").contains(\"|\"), split(col(\"metadata_raw\"), \"\\|\").getItem(0)) \\\n",
            "/tmp/ipython-input-927040372.py:8: SyntaxWarning: invalid escape sequence '\\('\n",
            "  when(col(\"metadata_raw\").startswith(\"(\"), regexp_extract(col(\"metadata_raw\"), \"\\('([^']+)',\\s*'([^']+)'\\)\", 2)) \\\n",
            "/tmp/ipython-input-927040372.py:9: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  .when(col(\"metadata_raw\").startswith(\"{\"), regexp_extract(col(\"metadata_raw\"), \"'level':\\s*'([^']+)'\", 1)) \\\n",
            "/tmp/ipython-input-927040372.py:10: SyntaxWarning: invalid escape sequence '\\|'\n",
            "  .when(col(\"metadata_raw\").contains(\"|\"), split(col(\"metadata_raw\"), \"\\|\").getItem(1)) \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+----------------------------+---------+------+--------+\n",
            "|course_id|course_name              |metadata_raw                |price_raw|domain|level   |\n",
            "+---------+-------------------------+----------------------------+---------+------+--------+\n",
            "|C001     |PySpark Mastery          |[Ljava.lang.Object;@7b0e82a2|₹9999    |NULL  |NULL    |\n",
            "|C002     |AI for Testers           |{level=Beginner, domain=QA} |8999     |      |        |\n",
            "|C003     |ML Foundations           |[Ljava.lang.Object;@73c75bf3|NULL     |NULL  |NULL    |\n",
            "|C004     |Data Engineering Bootcamp|Data|Advanced               |₹14999   |Data  |Advanced|\n",
            "+---------+-------------------------+----------------------------+---------+------+--------+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- metadata_raw: string (nullable = true)\n",
            " |-- price_raw: string (nullable = true)\n",
            " |-- domain: string (nullable = true)\n",
            " |-- level: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "df_price = df_courses_normalized.withColumn(\"price\", regexp_replace(col(\"price_raw\"), \"[^0-9]\", \"\").cast(IntegerType()))\n",
        "df_price.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6DNF0bGUCHV",
        "outputId": "89c6a681-c116-4dca-ec5d-71abff223e2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|course_id|         course_name|        metadata_raw|price_raw|domain|   level|price|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|     C001|     PySpark Mastery|[Ljava.lang.Objec...|    ₹9999|  NULL|    NULL| 9999|\n",
            "|     C002|      AI for Testers|{level=Beginner, ...|     8999|      |        | 8999|\n",
            "|     C003|      ML Foundations|[Ljava.lang.Objec...|     NULL|  NULL|    NULL| NULL|\n",
            "|     C004|Data Engineering ...|       Data|Advanced|   ₹14999|  Data|Advanced|14999|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "df_price = df_price.fillna({\"price\":0})\n",
        "df_price.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUfm9urSmGrT",
        "outputId": "21901b24-49c8-4762-a9dd-37bc249f089c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|course_id|         course_name|        metadata_raw|price_raw|domain|   level|price|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|     C001|     PySpark Mastery|[Ljava.lang.Objec...|    ₹9999|  NULL|    NULL| 9999|\n",
            "|     C002|      AI for Testers|{level=Beginner, ...|     8999|      |        | 8999|\n",
            "|     C003|      ML Foundations|[Ljava.lang.Objec...|     NULL|  NULL|    NULL|    0|\n",
            "|     C004|Data Engineering ...|       Data|Advanced|   ₹14999|  Data|Advanced|14999|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "df_courses_clean = df_price.select(\"course_id\", \"course_name\", \"domain\", \"level\", \"price\")\n",
        "df_courses_clean.show(truncate=False)\n",
        "df_courses_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYF6ECpbmo7U",
        "outputId": "d3c99206-494c-4c31-b387-5e7e5945402f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+------+--------+-----+\n",
            "|course_id|course_name              |domain|level   |price|\n",
            "+---------+-------------------------+------+--------+-----+\n",
            "|C001     |PySpark Mastery          |NULL  |NULL    |9999 |\n",
            "|C002     |AI for Testers           |      |        |8999 |\n",
            "|C003     |ML Foundations           |NULL  |NULL    |0    |\n",
            "|C004     |Data Engineering Bootcamp|Data  |Advanced|14999|\n",
            "+---------+-------------------------+------+--------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- domain: string (nullable = true)\n",
            " |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 3 — USER COURSE ENROLLMENTS (JOIN + BROADCAST)"
      ],
      "metadata": {
        "id": "zcCfgWeEm17U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "1. Normalize enrollment dates\n",
        "2. Identify invalid enrollments\n",
        "3. Join with users_df\n",
        "4. Join with courses_df\n",
        "5. Decide which table should be broadcast\n",
        "6. Prove your choice using explain(True)"
      ],
      "metadata": {
        "id": "eM_hd3mxm7VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "raw_enrollments = [\n",
        "(\"U001\",\"C001\",\"2024-01-05\"),\n",
        "(\"U002\",\"C002\",\"05/01/2024\"),\n",
        "(\"U003\",\"C001\",\"2024/01/06\"),\n",
        "(\"U004\",\"C003\",\"invalid_date\"),\n",
        "(\"U001\",\"C004\",\"2024-01-10\")\n",
        "]\n",
        "enroll_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"enrollment_date_raw\", StringType(), True)\n",
        "])\n",
        "df_enrollments_raw = spark.createDataFrame(raw_enrollments, enroll_schema)\n",
        "df_enrollments_raw.printSchema()\n",
        "df_enrollments_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptq2jdKsnCDz",
        "outputId": "d74f5196-bca5-4e57-80e5-5ac501b351e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- enrollment_date_raw: string (nullable = true)\n",
            "\n",
            "+-------+---------+-------------------+\n",
            "|user_id|course_id|enrollment_date_raw|\n",
            "+-------+---------+-------------------+\n",
            "|   U001|     C001|         2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|\n",
            "|   U003|     C001|         2024/01/06|\n",
            "|   U004|     C003|       invalid_date|\n",
            "|   U001|     C004|         2024-01-10|\n",
            "+-------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from pyspark.sql.functions import coalesce, col, to_date, when\n",
        "\n",
        "df_enrollments_raw = df_enrollments_raw.withColumn(\n",
        "    \"enrollment_date\",\n",
        "    coalesce(\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\d{4}-\\d{2}-\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy-MM-dd\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), to_date(col(\"enrollment_date_raw\"), \"dd/MM/yyyy\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\d{4}/\\d{2}/\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy/MM/dd\"))\n",
        "    )\n",
        ")\n",
        "df_enrollments_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eRj94Ufmt5A",
        "outputId": "e4ed942d-f17e-4193-a6f2-a2beb12ddd22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:7: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:7: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
            "/tmp/ipython-input-186298071.py:7: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  when(col(\"enrollment_date_raw\").rlike(\"^\\d{4}-\\d{2}-\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy-MM-dd\")),\n",
            "/tmp/ipython-input-186298071.py:8: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  when(col(\"enrollment_date_raw\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), to_date(col(\"enrollment_date_raw\"), \"dd/MM/yyyy\")),\n",
            "/tmp/ipython-input-186298071.py:9: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  when(col(\"enrollment_date_raw\").rlike(\"^\\d{4}/\\d{2}/\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy/MM/dd\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-------------------+---------------+\n",
            "|user_id|course_id|enrollment_date_raw|enrollment_date|\n",
            "+-------+---------+-------------------+---------------+\n",
            "|   U001|     C001|         2024-01-05|     2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|     2024-01-05|\n",
            "|   U003|     C001|         2024/01/06|     2024-01-06|\n",
            "|   U004|     C003|       invalid_date|           NULL|\n",
            "|   U001|     C004|         2024-01-10|     2024-01-10|\n",
            "+-------+---------+-------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "df_enrollments_processed = df_enrollments_raw.drop(\"enrollment_date_raw\")\n",
        "df_enriched = df_enrollments_processed.join(broadcast(df_courses_clean), on=\"course_id\", how=\"left\")\n",
        "df_enriched.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PtJmM9qnRGR",
        "outputId": "39f7d108-8594-4f14-d572-d392808acc91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------------+--------------------+------+--------+-----+\n",
            "|course_id|user_id|enrollment_date|         course_name|domain|   level|price|\n",
            "+---------+-------+---------------+--------------------+------+--------+-----+\n",
            "|     C001|   U001|     2024-01-05|     PySpark Mastery|  NULL|    NULL| 9999|\n",
            "|     C002|   U002|     2024-01-05|      AI for Testers|      |        | 8999|\n",
            "|     C001|   U003|     2024-01-06|     PySpark Mastery|  NULL|    NULL| 9999|\n",
            "|     C003|   U004|           NULL|      ML Foundations|  NULL|    NULL|    0|\n",
            "|     C004|   U001|     2024-01-10|Data Engineering ...|  Data|Advanced|14999|\n",
            "+---------+-------+---------------+--------------------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "df_enriched.show(truncate=False)\n",
        "df_enriched.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yyra21QinXRQ",
        "outputId": "23afef21-0ecd-4d8b-9e81-a72c120294cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------------+-------------------------+------+--------+-----+\n",
            "|course_id|user_id|enrollment_date|course_name              |domain|level   |price|\n",
            "+---------+-------+---------------+-------------------------+------+--------+-----+\n",
            "|C001     |U001   |2024-01-05     |PySpark Mastery          |NULL  |NULL    |9999 |\n",
            "|C002     |U002   |2024-01-05     |AI for Testers           |      |        |8999 |\n",
            "|C001     |U003   |2024-01-06     |PySpark Mastery          |NULL  |NULL    |9999 |\n",
            "|C003     |U004   |NULL           |ML Foundations           |NULL  |NULL    |0    |\n",
            "|C004     |U001   |2024-01-10     |Data Engineering Bootcamp|Data  |Advanced|14999|\n",
            "+---------+-------+---------------+-------------------------+------+--------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- enrollment_date: date (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- domain: string (nullable = true)\n",
            " |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "# Decision: Broadcast df_courses_clean\n",
        "# Reasoning: The `df_courses_clean` (course catalog) is expected to be significantly smaller than `df_enrollments_processed` (user enrollments).\n",
        "# Broadcasting the smaller table to all worker nodes during a join optimizes performance by avoiding a shuffle of the larger DataFrame and reducing network I/O.\n",
        "# This was already implemented in the previous join: `df_enrollments_processed.join(broadcast(df_courses_clean), on=\"course_id\", how=\"left\")`"
      ],
      "metadata": {
        "id": "QCFP5QqAndR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "df_enriched.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNgsxKQqnnbN",
        "outputId": "205f8ea8-5ebc-4621-ac7e-475e7dbdbf87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#190, course_id#191, enrollment_date#203]\n",
            ":  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            ":     +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                  +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enrollment_date: date, course_name: string, domain: string, level: string, price: int\n",
            "Project [course_id#191, user_id#190, enrollment_date#203, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103)\n",
            "   :- Project [user_id#190, course_id#191, enrollment_date#203]\n",
            "   :  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            "   :     +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "                  +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                     +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#191, user_id#190, enrollment_date#203, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#203]\n",
            "   :  +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "      +- Filter isnotnull(course_id#103)\n",
            "         +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [course_id#191, user_id#190, enrollment_date#203, course_name#104, domain#107, level#108, price#151]\n",
            "   +- BroadcastHashJoin [course_id#191], [course_id#103], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#203]\n",
            "      :  +- Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=316]\n",
            "         +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "            +- Filter isnotnull(course_id#103)\n",
            "               +- Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 4 — USER ACTIVITY LOGS (ARRAY + MAP)"
      ],
      "metadata": {
        "id": "8x6GFgOCn3qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "1. Normalize actions into ArrayType\n",
        "2. Normalize metadata into MapType\n",
        "3. Handle missing actions safely\n",
        "4. Explode actions and count frequency\n",
        "5. Produce activity_df"
      ],
      "metadata": {
        "id": "omMGeAaQn-KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "raw_activity = [\n",
        "(\"U001\",\"login,watch,logout\",\"{'device':'mobile','ip':'1.1.1.1'}\",120),\n",
        "(\"U002\",[\"login\",\"watch\"],\"device=laptop;ip=2.2.2.2\",90),\n",
        "(\"U003\",\"login|logout\",None,30),\n",
        "(\"U004\",None,\"{'device':'tablet'}\",60)\n",
        "]\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, when, split\n",
        "\n",
        "# 1. Defining the schema for the activity data\n",
        "activity_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"actions\", ArrayType(StringType()), True), # Normalize actions to ArrayType\n",
        "    StructField(\"properties\", StringType(), True),\n",
        "    StructField(\"duration\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# 2. Preprocess raw_activity to normalize the 'actions' field\n",
        "processed_raw_activity = []\n",
        "for user_id, actions_raw, properties, duration in raw_activity:\n",
        "    normalized_actions = None\n",
        "    if actions_raw is None:\n",
        "        normalized_actions = None\n",
        "    elif isinstance(actions_raw, list):\n",
        "        normalized_actions = actions_raw\n",
        "    elif isinstance(actions_raw, str):\n",
        "        if ',' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split(',')]\n",
        "        elif '|' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split('|')]\n",
        "\n",
        "    processed_raw_activity.append(Row(user_id=user_id, actions=normalized_actions, properties=properties, duration=duration))\n",
        "\n",
        "# 3. Creating the DataFrame\n",
        "df_activity = spark.createDataFrame(processed_raw_activity, activity_schema)\n",
        "\n",
        "df_activity.show(truncate=False)\n",
        "df_activity.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjdN_D-loHwq",
        "outputId": "11aaabce-6ebd-4e13-ee37-8ed338da9f9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+----------------------------------+--------+\n",
            "|user_id|actions               |properties                        |duration|\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{'device':'mobile','ip':'1.1.1.1'}|120     |\n",
            "|U002   |[login, watch]        |device=laptop;ip=2.2.2.2          |90      |\n",
            "|U003   |[login, logout]       |NULL                              |30      |\n",
            "|U004   |NULL                  |{'device':'tablet'}               |60      |\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from pyspark.sql.functions import col, when, from_json, regexp_replace, udf\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "\n",
        "# Define a UDF to parse custom key-value strings like \"device=laptop;ip=2.2.2.2\"\n",
        "def parse_custom_properties(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    try:\n",
        "        parts = s.split(';')\n",
        "        result_map = {}\n",
        "        for part in parts:\n",
        "            if '=' in part:\n",
        "                key, value = part.split('=', 1)\n",
        "                result_map[key.strip()] = value.strip()\n",
        "        return result_map\n",
        "    except Exception:\n",
        "        return None # Return None for malformed strings\n",
        "\n",
        "# Register the UDF\n",
        "parse_custom_properties_udf = udf(parse_custom_properties, MapType(StringType(), StringType()))\n",
        "\n",
        "df_activity_normalized = df_activity.withColumn(\n",
        "    \"properties\",\n",
        "    when(col(\"properties\").isNull(), None)\n",
        "    .when(col(\"properties\").startswith(\"{\"), # Check for JSON-like strings (start with '{')\n",
        "          from_json(regexp_replace(col(\"properties\"), \"'\", \"\\\"\"), MapType(StringType(), StringType())))\n",
        "    .otherwise(parse_custom_properties_udf(col(\"properties\"))) # Handle custom format for others\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFJttjzHon_P",
        "outputId": "fdeb86cf-25f0-4880-96a4-ac28fea8c80d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |NULL                  |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "from pyspark.sql.functions import col, when, array\n",
        "\n",
        "# Handle missing actions safely by replacing NULL with empty array\n",
        "df_activity_normalized = df_activity_normalized.withColumn(\n",
        "    \"actions\",\n",
        "    when(col(\"actions\").isNull(), array()).otherwise(col(\"actions\"))\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBhhwLaYowh5",
        "outputId": "3656a05e-c469-4248-8daf-2dbb9c3d7c6d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "from pyspark.sql.functions import explode, col, count\n",
        "\n",
        "# Explode the 'actions' array to create a new row for each action\n",
        "df_exploded_actions = df_activity_normalized.select(col(\"user_id\"), explode(col(\"actions\")).alias(\"action\"))\n",
        "\n",
        "# Count the frequency of each action\n",
        "action_frequency = df_exploded_actions.groupBy(\"action\").agg(count(\"action\").alias(\"frequency\"))\n",
        "\n",
        "# Show the results, ordered by frequency\n",
        "action_frequency.orderBy(col(\"frequency\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzlO13Slo4Xv",
        "outputId": "9e875dc9-7da8-494e-dcc2-19ff2c33600f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|action|frequency|\n",
            "+------+---------+\n",
            "| login|        3|\n",
            "| watch|        2|\n",
            "|logout|        2|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "activity_df = df_activity_normalized\n",
        "\n",
        "activity_df.show(truncate=False)\n",
        "activity_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWe8Y_dEo_xx",
        "outputId": "53faefc2-d276-4a5a-84ec-371f114296b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 5 — PAYMENTS (WINDOW + AGGREGATES)"
      ],
      "metadata": {
        "id": "uxiUq3oQpGVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "1. Convert dates properly\n",
        "2. Compute total spend per user (GroupBy)\n",
        "3. Compute running spend per user (Window)\n",
        "4. Rank users by total spend\n",
        "5. Compare GroupBy vs Window outputs"
      ],
      "metadata": {
        "id": "aI-hEmUtpKtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "raw_payments = [\n",
        "(\"U001\",\"2024-01-05\",9999),\n",
        "(\"U001\",\"2024-01-10\",14999),\n",
        "(\"U002\",\"2024-01-06\",8999),\n",
        "(\"U003\",\"2024-01-07\",0),\n",
        "(\"U004\",\"2024-01-08\",7999),\n",
        "(\"U001\",\"2024-01-15\",1999)\n",
        "]\n",
        "from pyspark.sql.functions import to_date, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define schema for raw_payments\n",
        "payment_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"payment_date_raw\", StringType(), True),\n",
        "    StructField(\"amount\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame from raw_payments\n",
        "df_payments = spark.createDataFrame(raw_payments, payment_schema)\n",
        "\n",
        "# Convert dates properly and drop the raw column\n",
        "df_payments = df_payments.withColumn(\"payment_date\", to_date(col(\"payment_date_raw\"), \"yyyy-MM-dd\")) \\\n",
        "                         .drop(\"payment_date_raw\")\n",
        "\n",
        "df_payments.printSchema()\n",
        "df_payments.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mrpfpDCpOoF",
        "outputId": "2efe7113-2040-4631-83ad-bdba5794d54e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- payment_date: date (nullable = true)\n",
            "\n",
            "+-------+------+------------+\n",
            "|user_id|amount|payment_date|\n",
            "+-------+------+------------+\n",
            "|   U001|  9999|  2024-01-05|\n",
            "|   U001| 14999|  2024-01-10|\n",
            "|   U002|  8999|  2024-01-06|\n",
            "|   U003|     0|  2024-01-07|\n",
            "|   U004|  7999|  2024-01-08|\n",
            "|   U001|  1999|  2024-01-15|\n",
            "+-------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "total_spend_per_user = df_payments.groupBy(\"user_id\").sum(\"amount\")\n",
        "total_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-O68go5pXzL",
        "outputId": "f213b1e2-9325-4092-d68d-13df7f8a4d7d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|user_id|sum(amount)|\n",
            "+-------+-----------+\n",
            "|   U002|       8999|\n",
            "|   U001|      26997|\n",
            "|   U004|       7999|\n",
            "|   U003|          0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"payment_date\")\n",
        "running_spend_per_user = df_payments.withColumn(\"running_spend\", sum(\"amount\").over(window_spec))\n",
        "running_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw65rrPmpdR5",
        "outputId": "5a91fe8d-fa67-4b18-a8d0-e803ce028bd8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------------+-------------+\n",
            "|user_id|amount|payment_date|running_spend|\n",
            "+-------+------+------------+-------------+\n",
            "|   U001|  9999|  2024-01-05|         9999|\n",
            "|   U001| 14999|  2024-01-10|        24998|\n",
            "|   U001|  1999|  2024-01-15|        26997|\n",
            "|   U002|  8999|  2024-01-06|         8999|\n",
            "|   U003|     0|  2024-01-07|            0|\n",
            "|   U004|  7999|  2024-01-08|         7999|\n",
            "+-------+------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "from pyspark.sql.functions import rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec_rank = Window.orderBy(desc(\"sum(amount)\"))\n",
        "\n",
        "ranked_users_by_total_spend = total_spend_per_user.withColumn(\"rank\", rank().over(window_spec_rank))\n",
        "\n",
        "ranked_users_by_total_spend.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKX-GPY8pix5",
        "outputId": "c8765deb-c02a-4bcd-b442-cc4480de49b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----+\n",
            "|user_id|sum(amount)|rank|\n",
            "+-------+-----------+----+\n",
            "|   U001|      26997|   1|\n",
            "|   U002|       8999|   2|\n",
            "|   U004|       7999|   3|\n",
            "|   U003|          0|   4|\n",
            "+-------+-----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "print(\"Total Spend Per User (GroupBy):\")\n",
        "total_spend_per_user.show()\n",
        "\n",
        "print(\"\\nRunning Spend Per User (Window Function):\")\n",
        "running_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VjUox3Gpod5",
        "outputId": "f58515d8-b819-48ec-cfd7-c575e34c0796"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Spend Per User (GroupBy):\n",
            "+-------+-----------+\n",
            "|user_id|sum(amount)|\n",
            "+-------+-----------+\n",
            "|   U002|       8999|\n",
            "|   U001|      26997|\n",
            "|   U004|       7999|\n",
            "|   U003|          0|\n",
            "+-------+-----------+\n",
            "\n",
            "\n",
            "Running Spend Per User (Window Function):\n",
            "+-------+------+------------+-------------+\n",
            "|user_id|amount|payment_date|running_spend|\n",
            "+-------+------+------------+-------------+\n",
            "|   U001|  9999|  2024-01-05|         9999|\n",
            "|   U001| 14999|  2024-01-10|        24998|\n",
            "|   U001|  1999|  2024-01-15|        26997|\n",
            "|   U002|  8999|  2024-01-06|         8999|\n",
            "|   U003|     0|  2024-01-07|            0|\n",
            "|   U004|  7999|  2024-01-08|         7999|\n",
            "+-------+------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 6 — PARTITIONS & PERFORMANCE"
      ],
      "metadata": {
        "id": "Tw8EnyTCpzhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "1. Check default partitions for all DataFrames\n",
        "2. Repartition enrollments by course_id\n",
        "3. Coalesce results before writing\n",
        "4. Write outputs and inspect file counts\n",
        "5. Explain why repartition caused shuffle"
      ],
      "metadata": {
        "id": "PlmGoLBVp72m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "dataframes_to_check = {\n",
        "    \"users_df\": df_clean_users,\n",
        "    \"courses_df\": df_courses_clean,\n",
        "    \"activity_df\": activity_df,\n",
        "    \"df_enrollments_processed\": df_enrollments_processed,\n",
        "    \"df_enriched\": df_enriched,\n",
        "    \"df_payments\": df_payments,\n",
        "    \"total_spend_per_user\": total_spend_per_user,\n",
        "    \"running_spend_per_user\": running_spend_per_user,\n",
        "    \"ranked_users_by_total_spend\": ranked_users_by_total_spend\n",
        "}\n",
        "\n",
        "print(\"--- Number of Partitions for DataFrames ---\")\n",
        "for df_name, df_obj in dataframes_to_check.items():\n",
        "    print(f\"DataFrame: {df_name}, Partitions: {df_obj.rdd.getNumPartitions()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WefUVCDjqA9x",
        "outputId": "56371145-928a-4093-ce50-b137a9171018"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Number of Partitions for DataFrames ---\n",
            "DataFrame: users_df, Partitions: 2\n",
            "DataFrame: courses_df, Partitions: 2\n",
            "DataFrame: activity_df, Partitions: 2\n",
            "DataFrame: df_enrollments_processed, Partitions: 2\n",
            "DataFrame: df_enriched, Partitions: 2\n",
            "DataFrame: df_payments, Partitions: 2\n",
            "DataFrame: total_spend_per_user, Partitions: 1\n",
            "DataFrame: running_spend_per_user, Partitions: 1\n",
            "DataFrame: ranked_users_by_total_spend, Partitions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "repartitioned_enrollments = df_enrollments_processed.repartition(\"course_id\")\n",
        "print(f\"Original df_enrollments_processed partitions: {df_enrollments_processed.rdd.getNumPartitions()}\")\n",
        "print(f\"Repartitioned enrollments partitions: {repartitioned_enrollments.rdd.getNumPartitions()}\")\n",
        "repartitioned_enrollments.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzSLb-58rSHI",
        "outputId": "e352b424-8d0a-475e-a613-e55f50bc9aad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original df_enrollments_processed partitions: 2\n",
            "Repartitioned enrollments partitions: 1\n",
            "+-------+---------+---------------+\n",
            "|user_id|course_id|enrollment_date|\n",
            "+-------+---------+---------------+\n",
            "|   U001|     C001|     2024-01-05|\n",
            "|   U002|     C002|     2024-01-05|\n",
            "|   U004|     C003|           NULL|\n",
            "|   U001|     C004|     2024-01-10|\n",
            "|   U003|     C001|     2024-01-06|\n",
            "+-------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "coalesced_enrollments = repartitioned_enrollments.coalesce(1)\n",
        "print(f\"Repartitioned enrollments partitions: {repartitioned_enrollments.rdd.getNumPartitions()}\")\n",
        "print(f\"Coalesced enrollments partitions: {coalesced_enrollments.rdd.getNumPartitions()}\")\n",
        "coalesced_enrollments.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBQmtdOSrXah",
        "outputId": "7629b4e0-ad03-4be7-ae53-99692df01778"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repartitioned enrollments partitions: 1\n",
            "Coalesced enrollments partitions: 1\n",
            "+-------+---------+---------------+\n",
            "|user_id|course_id|enrollment_date|\n",
            "+-------+---------+---------------+\n",
            "|   U001|     C001|     2024-01-05|\n",
            "|   U002|     C002|     2024-01-05|\n",
            "|   U004|     C003|           NULL|\n",
            "|   U001|     C004|     2024-01-10|\n",
            "|   U003|     C001|     2024-01-06|\n",
            "+-------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "output_path = \"/tmp/coalesced_enrollments_output\"\n",
        "\n",
        "# Clean up previous runs if any\n",
        "if os.path.exists(output_path):\n",
        "    shutil.rmtree(output_path)\n",
        "\n",
        "# Write the coalesced DataFrame to a single Parquet file\n",
        "# Using mode(\"overwrite\") to handle re-runs smoothly\n",
        "coalesced_enrollments.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "# Inspect file counts\n",
        "print(f\"Contents of {output_path}:\")\n",
        "files = os.listdir(output_path)\n",
        "for f in files:\n",
        "    print(f)\n",
        "\n",
        "# Count only the data files (e.g., .parquet files), excluding _SUCCESS and other metadata\n",
        "data_files = [f for f in files if f.endswith(\".parquet\")]\n",
        "print(f\"Number of data files written: {len(data_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoBJ8roQrc5h",
        "outputId": "91dcfd52-e728-449a-baed-636f3a39bfce"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /tmp/coalesced_enrollments_output:\n",
            "_SUCCESS\n",
            "._SUCCESS.crc\n",
            ".part-00000-8c546d88-ba30-4fa2-8997-0d2a908eae37-c000.snappy.parquet.crc\n",
            "part-00000-8c546d88-ba30-4fa2-8997-0d2a908eae37-c000.snappy.parquet\n",
            "Number of data files written: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartitioning, especially by a key (like 'course_id' in our case), inherently involves a 'shuffle' operation in Spark.\n",
        "# A shuffle is a costly operation where data needs to be redistributed across the network among different executors or even within the same executor.\n",
        "# When you repartition by 'course_id', Spark needs to ensure that all rows with the same 'course_id' are moved to the same new partition.\n",
        "# To achieve this, it must read all the data, hash the 'course_id' for each row, and then send the row to the appropriate target partition.\n",
        "# This involves:\n",
        "# 1. Serialization: Converting data to a format that can be sent over the network.\n",
        "# 2. Network I/O: Transferring data between different nodes.\n",
        "# 3. Deserialization: Converting data back into an in-memory format on the receiving side.\n",
        "# 4. Disk I/O: Often, data is spilled to disk during shuffling if it doesn't fit in memory.\n",
        "# This is in contrast to transformations like `filter` or `select`, which are 'narrow transformations' and can often be performed on data within existing partitions without moving it."
      ],
      "metadata": {
        "id": "9iamiD7JrjZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 7 — DAG & OPTIMIZATION"
      ],
      "metadata": {
        "id": "-TuXjppTrpeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "\n",
        "1. For each major transformation, run explain(True)\n",
        "2. Identify:\n",
        "*  Shuffles\n",
        "* Sorts\n",
        "*  Broadcast joins\n",
        "3. Identify one bad DAG\n",
        "4. Rewrite pipeline to improve it\n",
        "5. Justify improvements using physical plan"
      ],
      "metadata": {
        "id": "OiQVLAX3rt__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "dataframes_to_explain = {\n",
        "    \"df_clean_users\": df_clean_users,\n",
        "    \"df_courses_clean\": df_courses_clean,\n",
        "    \"activity_df\": activity_df,\n",
        "    \"df_enrollments_processed\": df_enrollments_processed,\n",
        "    \"df_enriched\": df_enriched,\n",
        "    \"df_payments\": df_payments,\n",
        "    \"total_spend_per_user\": total_spend_per_user,\n",
        "    \"running_spend_per_user\": running_spend_per_user,\n",
        "    \"ranked_users_by_total_spend\": ranked_users_by_total_spend\n",
        "}\n",
        "\n",
        "for df_name, df_obj in dataframes_to_explain.items():\n",
        "    print(f\"\\n--- Explain for DataFrame: {df_name} ---\")\n",
        "    df_obj.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2YC07CDsHCO",
        "outputId": "32b32d76-e1d4-46ff-e5cb-1494d1d38547"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Explain for DataFrame: df_clean_users ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['user_id, 'name, 'age, 'city, 'skills]\n",
            "+- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "   +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "      +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "         +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, name: string, age: int, city: string, skills: array<string>\n",
            "Project [user_id#0, name#64, age#21, city#3, skills#41]\n",
            "+- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "   +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "      +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "         +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) END AS age#21, city#3, CASE WHEN isnull(skills_raw#4) THEN [] WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "+- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) END AS age#21, city#3, CASE WHEN isnull(skills_raw#4) THEN [] WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "+- *(1) Scan ExistingRDD[user_id#0,name#1,age_raw#2,city#3,skills_raw#4]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_courses_clean ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['course_id, 'course_name, 'domain, 'level, 'price]\n",
            "+- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "   +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "            +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, course_name: string, domain: string, level: string, price: int\n",
            "Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "+- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "   +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "            +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "+- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "+- *(1) Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: activity_df ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(actions, CASE WHEN 'isNull('actions) THEN 'array() ELSE 'actions END, None)]\n",
            "+- Project [user_id#261, actions#262, CASE WHEN isnull(properties#263) THEN cast(null as map<string,string>) WHEN StartsWith(properties#263, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#263, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#263)#278 END AS properties#279, duration#264]\n",
            "   +- LogicalRDD [user_id#261, actions#262, properties#263, duration#264], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, actions: array<string>, properties: map<string,string>, duration: int\n",
            "Project [user_id#261, CASE WHEN isnull(actions#262) THEN cast(array() as array<string>) ELSE actions#262 END AS actions#294, properties#279, duration#264]\n",
            "+- Project [user_id#261, actions#262, CASE WHEN isnull(properties#263) THEN cast(null as map<string,string>) WHEN StartsWith(properties#263, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#263, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#263)#278 END AS properties#279, duration#264]\n",
            "   +- LogicalRDD [user_id#261, actions#262, properties#263, duration#264], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#261, CASE WHEN isnull(actions#262) THEN [] ELSE actions#262 END AS actions#294, CASE WHEN isnull(properties#263) THEN null WHEN StartsWith(properties#263, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#263, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#426 END AS properties#279, duration#264]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#263)#278], [pythonUDF0#426]\n",
            "   +- LogicalRDD [user_id#261, actions#262, properties#263, duration#264], false\n",
            "\n",
            "== Physical Plan ==\n",
            "Project [user_id#261, CASE WHEN isnull(actions#262) THEN [] ELSE actions#262 END AS actions#294, CASE WHEN isnull(properties#263) THEN null WHEN StartsWith(properties#263, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#263, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#426 END AS properties#279, duration#264]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#263)#278], [pythonUDF0#426]\n",
            "   +- *(1) Scan ExistingRDD[user_id#261,actions#262,properties#263,duration#264]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_enrollments_processed ---\n",
            "== Parsed Logical Plan ==\n",
            "Project [user_id#190, course_id#191, enrollment_date#203]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            "   +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, course_id: string, enrollment_date: date\n",
            "Project [user_id#190, course_id#191, enrollment_date#203]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            "   +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#203]\n",
            "+- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#203]\n",
            "+- *(1) Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_enriched ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#190, course_id#191, enrollment_date#203]\n",
            ":  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            ":     +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                  +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enrollment_date: date, course_name: string, domain: string, level: string, price: int\n",
            "Project [course_id#191, user_id#190, enrollment_date#203, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103)\n",
            "   :- Project [user_id#190, course_id#191, enrollment_date#203]\n",
            "   :  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            "   :     +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "                  +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                     +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#191, user_id#190, enrollment_date#203, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#203]\n",
            "   :  +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "      +- Filter isnotnull(course_id#103)\n",
            "         +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- *(2) Project [course_id#191, user_id#190, enrollment_date#203, course_name#104, domain#107, level#108, price#151]\n",
            "      +- *(2) BroadcastHashJoin [course_id#191], [course_id#103], LeftOuter, BuildRight, false\n",
            "         :- *(2) Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#203]\n",
            "         :  +- *(2) Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "         +- BroadcastQueryStage 0\n",
            "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=815]\n",
            "               +- *(1) Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "                  +- *(1) Filter isnotnull(course_id#103)\n",
            "                     +- *(1) Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "+- == Initial Plan ==\n",
            "   Project [course_id#191, user_id#190, enrollment_date#203, course_name#104, domain#107, level#108, price#151]\n",
            "   +- BroadcastHashJoin [course_id#191], [course_id#103], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#203]\n",
            "      :  +- Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=316]\n",
            "         +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "            +- Filter isnotnull(course_id#103)\n",
            "               +- Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_payments ---\n",
            "== Parsed Logical Plan ==\n",
            "Project [user_id#339, amount#341, payment_date#342]\n",
            "+- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "   +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, amount: int, payment_date: date\n",
            "Project [user_id#339, amount#341, payment_date#342]\n",
            "+- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "   +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#339, amount#341, cast(gettimestamp(payment_date_raw#340, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#342]\n",
            "+- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [user_id#339, amount#341, cast(gettimestamp(payment_date_raw#340, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#342]\n",
            "+- *(1) Scan ExistingRDD[user_id#339,payment_date_raw#340,amount#341]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: total_spend_per_user ---\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['user_id], ['user_id, unresolvedalias('sum(amount#341))]\n",
            "+- Project [user_id#339, amount#341, payment_date#342]\n",
            "   +- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "      +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, sum(amount): bigint\n",
            "Aggregate [user_id#339], [user_id#339, sum(amount#341) AS sum(amount)#357L]\n",
            "+- Project [user_id#339, amount#341, payment_date#342]\n",
            "   +- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "      +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [user_id#339], [user_id#339, sum(amount#341) AS sum(amount)#357L]\n",
            "+- Project [user_id#339, amount#341]\n",
            "   +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- *(2) HashAggregate(keys=[user_id#339], functions=[sum(amount#341)], output=[user_id#339, sum(amount)#357L])\n",
            "      +- AQEShuffleRead coalesced\n",
            "         +- ShuffleQueryStage 0\n",
            "            +- Exchange hashpartitioning(user_id#339, 200), ENSURE_REQUIREMENTS, [plan_id=873]\n",
            "               +- *(1) HashAggregate(keys=[user_id#339], functions=[partial_sum(amount#341)], output=[user_id#339, sum#361L])\n",
            "                  +- *(1) Project [user_id#339, amount#341]\n",
            "                     +- *(1) Scan ExistingRDD[user_id#339,payment_date_raw#340,amount#341]\n",
            "+- == Initial Plan ==\n",
            "   HashAggregate(keys=[user_id#339], functions=[sum(amount#341)], output=[user_id#339, sum(amount)#357L])\n",
            "   +- Exchange hashpartitioning(user_id#339, 200), ENSURE_REQUIREMENTS, [plan_id=862]\n",
            "      +- HashAggregate(keys=[user_id#339], functions=[partial_sum(amount#341)], output=[user_id#339, sum#361L])\n",
            "         +- Project [user_id#339, amount#341]\n",
            "            +- Scan ExistingRDD[user_id#339,payment_date_raw#340,amount#341]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: running_spend_per_user ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(running_spend, 'sum('amount) windowspecdefinition('user_id, 'payment_date ASC NULLS FIRST, unspecifiedframe$()), None)]\n",
            "+- Project [user_id#339, amount#341, payment_date#342]\n",
            "   +- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "      +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, amount: int, payment_date: date, running_spend: bigint\n",
            "Project [user_id#339, amount#341, payment_date#342, running_spend#368L]\n",
            "+- Project [user_id#339, amount#341, payment_date#342, running_spend#368L, running_spend#368L]\n",
            "   +- Window [sum(amount#341) windowspecdefinition(user_id#339, payment_date#342 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#368L], [user_id#339], [payment_date#342 ASC NULLS FIRST]\n",
            "      +- Project [user_id#339, amount#341, payment_date#342]\n",
            "         +- Project [user_id#339, amount#341, payment_date#342]\n",
            "            +- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "               +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [sum(amount#341) windowspecdefinition(user_id#339, payment_date#342 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#368L], [user_id#339], [payment_date#342 ASC NULLS FIRST]\n",
            "+- Project [user_id#339, amount#341, cast(gettimestamp(payment_date_raw#340, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#342]\n",
            "   +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- Window [sum(amount#341) windowspecdefinition(user_id#339, payment_date#342 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#368L], [user_id#339], [payment_date#342 ASC NULLS FIRST]\n",
            "      +- *(2) Sort [user_id#339 ASC NULLS FIRST, payment_date#342 ASC NULLS FIRST], false, 0\n",
            "         +- AQEShuffleRead coalesced\n",
            "            +- ShuffleQueryStage 0\n",
            "               +- Exchange hashpartitioning(user_id#339, 200), ENSURE_REQUIREMENTS, [plan_id=908]\n",
            "                  +- *(1) Project [user_id#339, amount#341, cast(gettimestamp(payment_date_raw#340, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#342]\n",
            "                     +- *(1) Scan ExistingRDD[user_id#339,payment_date_raw#340,amount#341]\n",
            "+- == Initial Plan ==\n",
            "   Window [sum(amount#341) windowspecdefinition(user_id#339, payment_date#342 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#368L], [user_id#339], [payment_date#342 ASC NULLS FIRST]\n",
            "   +- Sort [user_id#339 ASC NULLS FIRST, payment_date#342 ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(user_id#339, 200), ENSURE_REQUIREMENTS, [plan_id=898]\n",
            "         +- Project [user_id#339, amount#341, cast(gettimestamp(payment_date_raw#340, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#342]\n",
            "            +- Scan ExistingRDD[user_id#339,payment_date_raw#340,amount#341]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: ranked_users_by_total_spend ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(rank, 'rank() windowspecdefinition('sum(amount) DESC NULLS LAST, unspecifiedframe$()), None)]\n",
            "+- Aggregate [user_id#339], [user_id#339, sum(amount#341) AS sum(amount)#357L]\n",
            "   +- Project [user_id#339, amount#341, payment_date#342]\n",
            "      +- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "         +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, sum(amount): bigint, rank: int\n",
            "Project [user_id#339, sum(amount)#357L, rank#384]\n",
            "+- Project [user_id#339, sum(amount)#357L, rank#384, rank#384]\n",
            "   +- Window [rank(sum(amount)#357L) windowspecdefinition(sum(amount)#357L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#384], [sum(amount)#357L DESC NULLS LAST]\n",
            "      +- Project [user_id#339, sum(amount)#357L]\n",
            "         +- Aggregate [user_id#339], [user_id#339, sum(amount#341) AS sum(amount)#357L]\n",
            "            +- Project [user_id#339, amount#341, payment_date#342]\n",
            "               +- Project [user_id#339, payment_date_raw#340, amount#341, to_date(payment_date_raw#340, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#342]\n",
            "                  +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [rank(sum(amount)#357L) windowspecdefinition(sum(amount)#357L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#384], [sum(amount)#357L DESC NULLS LAST]\n",
            "+- Aggregate [user_id#339], [user_id#339, sum(amount#341) AS sum(amount)#357L]\n",
            "   +- Project [user_id#339, amount#341]\n",
            "      +- LogicalRDD [user_id#339, payment_date_raw#340, amount#341], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 2\n",
            "   +- Window [rank(sum(amount)#357L) windowspecdefinition(sum(amount)#357L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#384], [sum(amount)#357L DESC NULLS LAST]\n",
            "      +- *(3) Sort [sum(amount)#357L DESC NULLS LAST], false, 0\n",
            "         +- ShuffleQueryStage 1\n",
            "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=991]\n",
            "               +- *(2) HashAggregate(keys=[user_id#339], functions=[sum(amount#341)], output=[user_id#339, sum(amount)#357L])\n",
            "                  +- AQEShuffleRead coalesced\n",
            "                     +- ShuffleQueryStage 0\n",
            "                        +- Exchange hashpartitioning(user_id#339, 200), ENSURE_REQUIREMENTS, [plan_id=966]\n",
            "                           +- *(1) HashAggregate(keys=[user_id#339], functions=[partial_sum(amount#341)], output=[user_id#339, sum#361L])\n",
            "                              +- *(1) Project [user_id#339, amount#341]\n",
            "                                 +- *(1) Scan ExistingRDD[user_id#339,payment_date_raw#340,amount#341]\n",
            "+- == Initial Plan ==\n",
            "   Window [rank(sum(amount)#357L) windowspecdefinition(sum(amount)#357L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#384], [sum(amount)#357L DESC NULLS LAST]\n",
            "   +- Sort [sum(amount)#357L DESC NULLS LAST], false, 0\n",
            "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=951]\n",
            "         +- HashAggregate(keys=[user_id#339], functions=[sum(amount#341)], output=[user_id#339, sum(amount)#357L])\n",
            "            +- Exchange hashpartitioning(user_id#339, 200), ENSURE_REQUIREMENTS, [plan_id=948]\n",
            "               +- HashAggregate(keys=[user_id#339], functions=[partial_sum(amount#341)], output=[user_id#339, sum#361L])\n",
            "                  +- Project [user_id#339, amount#341]\n",
            "                     +- Scan ExistingRDD[user_id#339,payment_date_raw#340,amount#341]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "df_clean_users, df_courses_clean, df_enrollments_processed, df_payments: These DataFrames generally show straightforward Project and Scan ExistingRDD operations. There are no explicit shuffles, sorts, or broadcast joins at these initial transformation stages.\n",
        "\n",
        "activity_df: This DataFrame uses a Python UDF. While UDFs introduce their own overhead, the core Spark physical plan for this DataFrame doesn't show shuffles, sorts, or broadcast joins.\n",
        "\n",
        "df_enriched: This is where we clearly see a Broadcast Join! The physical plan contains BroadcastHashJoin (indicating the join strategy) and BroadcastExchange (confirming that df_courses_clean, the smaller table, was sent to all worker nodes).\n",
        "\n",
        "total_spend_per_user: This DataFrame involves a groupBy operation, which necessitates a Shuffle. You'll see Exchange hashpartitioning in its physical plan.\n",
        "\n",
        "running_spend_per_user: This DataFrame uses a window function with partitionBy and orderBy. This leads to both a Shuffle (Exchange hashpartitioning to group by user_id) and a Sort (Sort to order by payment_date within each user's partition).\n",
        "\n",
        "ranked_users_by_total_spend: This DataFrame is notable for multiple performance-impacting operations. It involves:\n",
        "\n",
        "Shuffles: An Exchange hashpartitioning for the initial aggregation (calculating total spend) and then critically, an Exchange SinglePartition to gather all aggregated data onto a single executor before global ranking. Sorts: A Sort operation is performed on this single partition to establish the global order required for the rank window function."
      ],
      "metadata": {
        "id": "573zcD26sh3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "# Bad DAG identified in `ranked_users_by_total_spend`:\n",
        "# The physical plan for `ranked_users_by_total_spend` includes an `Exchange SinglePartition` followed by a global `Sort`.\n",
        "# This means that after computing the total spend per user (which already involves a shuffle for aggregation),\n",
        "# Spark then gathers ALL the aggregated data into a single partition (`Exchange SinglePartition`) to perform a global sort (`Sort`).\n",
        "# This design choice, while correct for achieving a global rank, is highly inefficient and becomes a major bottleneck\n",
        "# for large datasets as it eliminates parallelism and forces all data processing onto a single executor."
      ],
      "metadata": {
        "id": "TUozCVHVsmze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "good_df = df_enrollments_processed \\\n",
        ".filter(col(\"enrollment_date\").isNotNull())\\\n",
        ".join(broadcast(df_clean_users), \"user_id\")\\\n",
        ".groupBy(\"course_id\")\\\n",
        ".count()\n",
        "good_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaQsExV1sSo2",
        "outputId": "0ee668f6-b652-4653-9116-1ebfe6299c36"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|course_id|count|\n",
            "+---------+-----+\n",
            "|     C001|    2|\n",
            "|     C002|    1|\n",
            "|     C004|    1|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "good_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOVTDd91s1jX",
        "outputId": "1f957c4a-6f29-4adb-c8d8-335aa14fa7da"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['course_id], ['course_id, 'count(1) AS count#489]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date#203, name#64, age#21, city#3, skills#41]\n",
            "   +- Join Inner, (user_id#190 = user_id#0)\n",
            "      :- Filter isnotnull(enrollment_date#203)\n",
            "      :  +- Project [user_id#190, course_id#191, enrollment_date#203]\n",
            "      :     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            "      :        +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- Project [user_id#0, name#64, age#21, city#3, skills#41]\n",
            "            +- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "               +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "                  +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "                     +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, count: bigint\n",
            "Aggregate [course_id#191], [course_id#191, count(1) AS count#489L]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date#203, name#64, age#21, city#3, skills#41]\n",
            "   +- Join Inner, (user_id#190 = user_id#0)\n",
            "      :- Filter isnotnull(enrollment_date#203)\n",
            "      :  +- Project [user_id#190, course_id#191, enrollment_date#203]\n",
            "      :     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#203]\n",
            "      :        +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- Project [user_id#0, name#64, age#21, city#3, skills#41]\n",
            "            +- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "               +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "                  +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "                     +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [course_id#191], [course_id#191, count(1) AS count#489L]\n",
            "+- Project [course_id#191]\n",
            "   +- Join Inner, (user_id#190 = user_id#0), rightHint=(strategy=broadcast)\n",
            "      :- Project [user_id#190, course_id#191]\n",
            "      :  +- Filter (isnotnull(coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END)) AND isnotnull(user_id#190))\n",
            "      :     +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "      +- Project [user_id#0]\n",
            "         +- Filter isnotnull(user_id#0)\n",
            "            +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[course_id#191], functions=[count(1)], output=[course_id#191, count#489L])\n",
            "   +- Exchange hashpartitioning(course_id#191, 200), ENSURE_REQUIREMENTS, [plan_id=1686]\n",
            "      +- HashAggregate(keys=[course_id#191], functions=[partial_count(1)], output=[course_id#191, count#501L])\n",
            "         +- Project [course_id#191]\n",
            "            +- BroadcastHashJoin [user_id#190], [user_id#0], Inner, BuildRight, false\n",
            "               :- Project [user_id#190, course_id#191]\n",
            "               :  +- Filter (isnotnull(coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END)) AND isnotnull(user_id#190))\n",
            "               :     +- Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1681]\n",
            "                  +- Project [user_id#0]\n",
            "                     +- Filter isnotnull(user_id#0)\n",
            "                        +- Scan ExistingRDD[user_id#0,name#1,age_raw#2,city#3,skills_raw#4]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}