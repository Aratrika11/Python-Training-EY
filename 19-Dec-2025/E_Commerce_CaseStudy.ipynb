{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 1 — CUSTOMER MASTER (CORRUPTED)"
      ],
      "metadata": {
        "id": "veZxLg635shw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iKLrc5q4x5V"
      },
      "outputs": [],
      "source": [
        "raw_customers = [\n",
        "(\"C001\",\"Rahul\",\"29\",\"Bangalore\",\"Electronics,Fashion\"),\n",
        "(\"C002\",\"Sneha\",\"Thirty Two\",\"Delhi\",\"Fashion\"),\n",
        "(\"C003\",\"Aman\",None,\"Mumbai\",[\"Home\",\"Electronics\"]),\n",
        "(\"C004\",\"Pallavi\",\"27\",\"Pune\",\"Electronics|Beauty\"),\n",
        "\n",
        "(\"C005\",\"\", \"35\",\"Chennai\",None)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 2 — SELLER MASTER"
      ],
      "metadata": {
        "id": "RqxG8Fd25y5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_sellers = [\n",
        "(\"S001\",\"TechWorld\",\"Electronics\",\"2019-06-01\"),\n",
        "(\"S002\",\"FashionHub\",\"Fashion\",\"01/07/2020\"),\n",
        "(\"S003\",\"HomeEssentials\",\"Home\",\"2018/09/15\"),\n",
        "(\"S004\",\"BeautyStore\",\"Beauty\",\"invalid_date\")\n",
        "]"
      ],
      "metadata": {
        "id": "Xla0bcfa57wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 3 — PRODUCT CATALOG"
      ],
      "metadata": {
        "id": "8nco2UM56BRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_products = [\n",
        "(\"P001\",\"Laptop\",\"Electronics\",\"S001\",\"55000\"),\n",
        "(\"P002\",\"Headphones\",\"Electronics\",\"S001\",\"2500\"),\n",
        "(\"P003\",\"T-Shirt\",\"Fashion\",\"S002\",\"1200\"),\n",
        "(\"P004\",\"Sofa\",\"Home\",\"S003\",\"45000\"),\n",
        "(\"P005\",\"Face Cream\",\"Beauty\",\"S004\",\"800\")\n",
        "]"
      ],
      "metadata": {
        "id": "di63CdT76Jc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 4 — ORDERS DATA"
      ],
      "metadata": {
        "id": "5FlUKNMu6UL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_orders = [\n",
        "(\"O001\",\"C001\",\"P001\",\"2024-01-05\",\"Delivered\",\"55000\"),\n",
        "(\"O002\",\"C002\",\"P003\",\"05/01/2024\",\"Cancelled\",\"0\"),\n",
        "(\"O003\",\"C003\",\"P004\",\"2024/01/06\",\"Delivered\",\"45000\"),\n",
        "(\"O004\",\"C004\",\"P005\",\"invalid_date\",\"Delivered\",\"800\"),\n",
        "(\"O005\",\"C001\",\"P002\",\"2024-01-10\",\"Delivered\",\"2500\"),\n",
        "(\"O006\",\"C005\",\"P003\",\"2024-01-12\",\"Delivered\",\"1200\")\n",
        "]"
      ],
      "metadata": {
        "id": "ZoTusaIS6Xu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 5 — CUSTOMER ACTIVITY LOGS"
      ],
      "metadata": {
        "id": "6yddJIv16bC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        "(\"C001\",\"search,view,add_to_cart\",\"{'device':'mobile'}\",180),\n",
        "(\"C002\",[\"search\",\"view\"],\"device=laptop\",90),\n",
        "(\"C003\",\"search|view|purchase\",None,120),\n",
        "(\"C004\",None,\"{'device':'tablet'}\",60),\n",
        "(\"C005\",\"search\",\"{'device':'mobile'}\",30)\n",
        "]"
      ],
      "metadata": {
        "id": "xYKcyBGB6eP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART A — DATA CLEANING & STRUCTURING\n",
        "\n",
        "1. Design explicit schemas for all datasets\n",
        "2. Normalize:\n",
        "\n",
        "- Age\n",
        "- Prices\n",
        "- Dates\n",
        "\n",
        "3. Convert interests and actions into arrays\n",
        "4. Handle missing and invalid records gracefully\n",
        "5. Produce clean DataFrames:\n",
        "\n",
        "- customers_df\n",
        "- sellers_df\n",
        "- products_df\n",
        "- orders_df\n",
        "- activity_df"
      ],
      "metadata": {
        "id": "jssxpWCV6i8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1"
      ],
      "metadata": {
        "id": "-SiNK-0k7EQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"E-CommerceAnalytics\").getOrCreate()"
      ],
      "metadata": {
        "id": "XI0vrBNS7Fn8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_customers = [\n",
        "(\"C001\",\"Rahul\",\"29\",\"Bangalore\",\"Electronics,Fashion\"),\n",
        "(\"C002\",\"Sneha\",\"Thirty Two\",\"Delhi\",\"Fashion\"),\n",
        "(\"C003\",\"Aman\",None,\"Mumbai\",[\"Home\",\"Electronics\"]),\n",
        "(\"C004\",\"Pallavi\",\"27\",\"Pune\",\"Electronics|Beauty\"),\n",
        "\n",
        "(\"C005\",\"\", \"35\",\"Chennai\",None)\n",
        "]\n",
        "customer_schema = StructType([\n",
        "StructField(\"customer_id\", StringType(), True),\n",
        "StructField(\"name\", StringType(), True),\n",
        "StructField(\"age\", StringType(), True),\n",
        "StructField(\"city\", StringType(), True),\n",
        "StructField(\"interests\", StringType(), True)\n",
        "])\n",
        "raw_customers_df = spark.createDataFrame(raw_customers, schema=customer_schema)"
      ],
      "metadata": {
        "id": "GDKEntwhQdkD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_sellers = [\n",
        "(\"S001\",\"TechWorld\",\"Electronics\",\"2019-06-01\"),\n",
        "(\"S002\",\"FashionHub\",\"Fashion\",\"01/07/2020\"),\n",
        "(\"S003\",\"HomeEssentials\",\"Home\",\"2018/09/15\"),\n",
        "(\"S004\",\"BeautyStore\",\"Beauty\",\"invalid_date\")\n",
        "]\n",
        "seller_schema = StructType([\n",
        "StructField(\"seller_id\", StringType(), True),\n",
        "StructField(\"seller_name\", StringType(), True),\n",
        "StructField(\"category\", StringType(), True),\n",
        "StructField(\"start_date\", StringType(), True)\n",
        "])\n",
        "raw_sellers_df = spark.createDataFrame(raw_sellers, schema=seller_schema)"
      ],
      "metadata": {
        "id": "AcVKfOm0QkJu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_products = [\n",
        "(\"P001\",\"Laptop\",\"Electronics\",\"S001\",\"55000\"),\n",
        "(\"P002\",\"Headphones\",\"Electronics\",\"S001\",\"2500\"),\n",
        "(\"P003\",\"T-Shirt\",\"Fashion\",\"S002\",\"1200\"),\n",
        "(\"P004\",\"Sofa\",\"Home\",\"S003\",\"45000\"),\n",
        "(\"P005\",\"Face Cream\",\"Beauty\",\"S004\",\"800\")\n",
        "]\n",
        "product_schema = StructType([\n",
        "StructField(\"product_id\", StringType(), True),\n",
        "StructField(\"product\", StringType(), True),\n",
        "StructField(\"category\", StringType(), True),\n",
        "StructField(\"seller_id\", StringType(), True),\n",
        "StructField(\"start_date\", StringType(), True)\n",
        "])\n",
        "raw_products_df = spark.createDataFrame(raw_products, schema=product_schema)"
      ],
      "metadata": {
        "id": "E0FipYlPQ9iS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_orders = [\n",
        "(\"O001\",\"C001\",\"P001\",\"2024-01-05\",\"Delivered\",\"55000\"),\n",
        "(\"O002\",\"C002\",\"P003\",\"05/01/2024\",\"Cancelled\",\"0\"),\n",
        "(\"O003\",\"C003\",\"P004\",\"2024/01/06\",\"Delivered\",\"45000\"),\n",
        "(\"O004\",\"C004\",\"P005\",\"invalid_date\",\"Delivered\",\"800\"),\n",
        "(\"O005\",\"C001\",\"P002\",\"2024-01-10\",\"Delivered\",\"2500\"),\n",
        "(\"O006\",\"C005\",\"P003\",\"2024-01-12\",\"Delivered\",\"1200\")\n",
        "]\n",
        "order_schema = StructType([\n",
        "StructField(\"order_id\", StringType(), True),\n",
        "StructField(\"customer_id\", StringType(), True),\n",
        "StructField(\"product_id\", StringType(), True),\n",
        "StructField(\"order_date\", StringType(), True),\n",
        "StructField(\"status\", StringType(), True),\n",
        "StructField(\"amount\", StringType(), True)\n",
        "])\n",
        "raw_orders_df = spark.createDataFrame(raw_orders, schema=order_schema)"
      ],
      "metadata": {
        "id": "y7g2hcPWRK3B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        "(\"C001\",\"search,view,add_to_cart\",\"{'device':'mobile'}\",180),\n",
        "(\"C002\",[\"search\",\"view\"],\"device=laptop\",90),\n",
        "(\"C003\",\"search|view|purchase\",None,120),\n",
        "(\"C004\",None,\"{'device':'tablet'}\",60),\n",
        "(\"C005\",\"search\",\"{'device':'mobile'}\",30)\n",
        "]\n",
        "activity_schema = StructType([\n",
        "StructField(\"customer_id\", StringType(), True),\n",
        "StructField(\"actions\", StringType(), True),\n",
        "StructField(\"metadata\", StringType(), True),\n",
        "StructField(\"duration\", IntegerType(), True)\n",
        "])\n",
        "raw_activity_df = spark.createDataFrame(raw_activity, schema=activity_schema)"
      ],
      "metadata": {
        "id": "EQxcXYPGRSbU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from pyspark.sql.functions import regexp_extract, col, when\n",
        "\n",
        "customers_df = raw_customers_df \\\n",
        "    .withColumn(\"age\", when(regexp_extract(\"age\", \"\\d+\", 0) == \"\", None)\n",
        "                .otherwise(regexp_extract(\"age\", \"\\d+\", 0)).cast(\"int\")) \\\n",
        "    .withColumn(\"name\", when(col(\"name\") == \"\", None).otherwise(col(\"name\")))\n",
        "\n",
        "customers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jutEueQcRgCW",
        "outputId": "506f89ce-0cb9-4c06-c1e9-e8f34c226b1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\d'\n",
            "/tmp/ipython-input-3504750777.py:5: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  .withColumn(\"age\", when(regexp_extract(\"age\", \"\\d+\", 0) == \"\", None)\n",
            "/tmp/ipython-input-3504750777.py:6: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  .otherwise(regexp_extract(\"age\", \"\\d+\", 0)).cast(\"int\")) \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+----+---------+-------------------+\n",
            "|customer_id|   name| age|     city|          interests|\n",
            "+-----------+-------+----+---------+-------------------+\n",
            "|       C001|  Rahul|  29|Bangalore|Electronics,Fashion|\n",
            "|       C002|  Sneha|NULL|    Delhi|            Fashion|\n",
            "|       C003|   Aman|NULL|   Mumbai|[Home, Electronics]|\n",
            "|       C004|Pallavi|  27|     Pune| Electronics|Beauty|\n",
            "|       C005|   NULL|  35|  Chennai|               NULL|\n",
            "+-----------+-------+----+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "products_df = raw_products_df.withColumnRenamed(\"start_date\", \"price\").withColumn(\"price\", col(\"price\").cast(\"int\"))\n",
        "products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvYrhd3MRn1_",
        "outputId": "7a93cb2d-4447-4ff4-959a-12552c7f72da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------+---------+-----+\n",
            "|product_id|   product|   category|seller_id|price|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|      P001|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|      Sofa|       Home|     S003|45000|\n",
            "|      P005|Face Cream|     Beauty|     S004|  800|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df =raw_orders_df.withColumn(\"order_date\",\n",
        "                                    coalesce(\n",
        "                                        to_date(try_to_timestamp(col(\"order_date\"),lit(\"yyyy-MM-dd\"))),\n",
        "                                        to_date(try_to_timestamp(col(\"order_date\"),lit(\"dd/MM/yyyy\"))),\n",
        "                                        to_date(try_to_timestamp(col(\"order_date\"),lit(\"yyyy/MM/dd\")))\n",
        "                                    )\n",
        "                                   )\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjhB3in9RqCp",
        "outputId": "d1d60b13-0fa0-4589-a5b6-ab09a3738891"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+----------+---------+------+\n",
            "|order_id|customer_id|product_id|order_date|   status|amount|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "|    O001|       C001|      P001|2024-01-05|Delivered| 55000|\n",
            "|    O002|       C002|      P003|2024-01-05|Cancelled|     0|\n",
            "|    O003|       C003|      P004|2024-01-06|Delivered| 45000|\n",
            "|    O004|       C004|      P005|      NULL|Delivered|   800|\n",
            "|    O005|       C001|      P002|2024-01-10|Delivered|  2500|\n",
            "|    O006|       C005|      P003|2024-01-12|Delivered|  1200|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "from pyspark.sql.functions import split, regexp_replace\n",
        "\n",
        "customers_df = customers_df.withColumn(\n",
        "    \"interests\",\n",
        "    split(regexp_replace(\"interests\", \"[|]\", \",\"), \",\")\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import split, regexp_replace\n",
        "\n",
        "activity_df = raw_activity_df.withColumn(\n",
        "    \"actions\",\n",
        "    split(regexp_replace(\"actions\", \"[|]\", \",\"), \",\")\n",
        ")"
      ],
      "metadata": {
        "id": "KO2pUg-nRuEe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "from pyspark.sql.functions import col, to_date, coalesce, split, lit, array_remove, try_to_timestamp\n",
        "\n",
        "# Make an empty string array: split(\"\", \",\") -> [\"\"] then remove \"\" -> []\n",
        "empty_string_array = array_remove(split(lit(\"\"), \",\"), \"\")\n",
        "\n",
        "customers_df = customers_df.withColumn(\n",
        "    \"interests\",\n",
        "    coalesce(col(\"interests\"), empty_string_array)\n",
        ")\n",
        "\n",
        "orders_df = orders_df.filter(col(\"order_date\").isNotNull())\n",
        "\n",
        "sellers_df = raw_sellers_df.withColumn(\n",
        "    \"start_date\",\n",
        "    coalesce(\n",
        "        to_date(try_to_timestamp(col(\"start_date\"), lit(\"yyyy-MM-dd\"))),\n",
        "        to_date(try_to_timestamp(col(\"start_date\"), lit(\"dd/MM/yyyy\"))),\n",
        "        to_date(try_to_timestamp(col(\"start_date\"), lit(\"yyyy/MM/dd\")))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "J6xYHZkhR5wM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "customers_df.printSchema()\n",
        "customers_df.show()\n",
        "sellers_df.printSchema()\n",
        "sellers_df.show()\n",
        "products_df.printSchema()\n",
        "products_df.show()\n",
        "orders_df.printSchema()\n",
        "orders_df.show()\n",
        "activity_df.printSchema()\n",
        "activity_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4GqUh7LR_qe",
        "outputId": "7b01b9ff-e63a-4b56-9f8d-dbb7b6bd1281"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = false)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n",
            "+-----------+-------+----+---------+--------------------+\n",
            "|customer_id|   name| age|     city|           interests|\n",
            "+-----------+-------+----+---------+--------------------+\n",
            "|       C001|  Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|  Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C003|   Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C004|Pallavi|  27|     Pune|[Electronics, Bea...|\n",
            "|       C005|   NULL|  35|  Chennai|                  []|\n",
            "+-----------+-------+----+---------+--------------------+\n",
            "\n",
            "root\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- seller_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- start_date: date (nullable = true)\n",
            "\n",
            "+---------+--------------+-----------+----------+\n",
            "|seller_id|   seller_name|   category|start_date|\n",
            "+---------+--------------+-----------+----------+\n",
            "|     S001|     TechWorld|Electronics|2019-06-01|\n",
            "|     S002|    FashionHub|    Fashion|2020-07-01|\n",
            "|     S003|HomeEssentials|       Home|2018-09-15|\n",
            "|     S004|   BeautyStore|     Beauty|      NULL|\n",
            "+---------+--------------+-----------+----------+\n",
            "\n",
            "root\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|product_id|   product|   category|seller_id|price|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|      P001|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|      Sofa|       Home|     S003|45000|\n",
            "|      P005|Face Cream|     Beauty|     S004|  800|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            "\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "|order_id|customer_id|product_id|order_date|   status|amount|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "|    O001|       C001|      P001|2024-01-05|Delivered| 55000|\n",
            "|    O002|       C002|      P003|2024-01-05|Cancelled|     0|\n",
            "|    O003|       C003|      P004|2024-01-06|Delivered| 45000|\n",
            "|    O005|       C001|      P002|2024-01-10|Delivered|  2500|\n",
            "|    O006|       C005|      P003|2024-01-12|Delivered|  1200|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            " |-- metadata: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n",
            "+-----------+--------------------+-------------------+--------+\n",
            "|customer_id|             actions|           metadata|duration|\n",
            "+-----------+--------------------+-------------------+--------+\n",
            "|       C001|[search, view, ad...|{'device':'mobile'}|     180|\n",
            "|       C002|   [[search,  view]]|      device=laptop|      90|\n",
            "|       C003|[search, view, pu...|               NULL|     120|\n",
            "|       C004|                NULL|{'device':'tablet'}|      60|\n",
            "|       C005|            [search]|{'device':'mobile'}|      30|\n",
            "+-----------+--------------------+-------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART B — DATA INTEGRATION (JOINS)\n",
        "\n",
        "6. Join orders with products\n",
        "7. Join products with sellers\n",
        "8. Join orders with customers\n",
        "9. Decide which table(s) should be broadcast\n",
        "10. Prove your decision using explain(True)\n",
        "11. Eliminate orphan records"
      ],
      "metadata": {
        "id": "LZqxbiOzSPqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "orders_products_df  = orders_df.join(products_df, \"product_id\", \"inner\")\n",
        "orders_products_df.show()\n",
        "\n",
        "#7\n",
        "products_seller_df  = products_df.join(broadcast(sellers_df), \"seller_id\", \"inner\")\n",
        "products_seller_df.show()\n",
        "\n",
        "#8\n",
        "orders_customers_df  = orders_df.join(customers_df, \"customer_id\", \"inner\")\n",
        "orders_customers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8yP_YsUS6oa",
        "outputId": "d8c20422-a747-49ad-e54c-b8d12298cbb0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "|product_id|order_id|customer_id|order_date|   status|amount|   product|   category|seller_id|price|\n",
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "|      P001|    O001|       C001|2024-01-05|Delivered| 55000|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|    O005|       C001|2024-01-10|Delivered|  2500|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|    O002|       C002|2024-01-05|Cancelled|     0|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P003|    O006|       C005|2024-01-12|Delivered|  1200|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|    O003|       C003|2024-01-06|Delivered| 45000|      Sofa|       Home|     S003|45000|\n",
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "\n",
            "+---------+----------+----------+-----------+-----+--------------+-----------+----------+\n",
            "|seller_id|product_id|   product|   category|price|   seller_name|   category|start_date|\n",
            "+---------+----------+----------+-----------+-----+--------------+-----------+----------+\n",
            "|     S001|      P001|    Laptop|Electronics|55000|     TechWorld|Electronics|2019-06-01|\n",
            "|     S001|      P002|Headphones|Electronics| 2500|     TechWorld|Electronics|2019-06-01|\n",
            "|     S002|      P003|   T-Shirt|    Fashion| 1200|    FashionHub|    Fashion|2020-07-01|\n",
            "|     S003|      P004|      Sofa|       Home|45000|HomeEssentials|       Home|2018-09-15|\n",
            "|     S004|      P005|Face Cream|     Beauty|  800|   BeautyStore|     Beauty|      NULL|\n",
            "+---------+----------+----------+-----------+-----+--------------+-----------+----------+\n",
            "\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|customer_id|order_id|product_id|order_date|   status|amount| name| age|     city|           interests|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|       C001|    O001|      P001|2024-01-05|Delivered| 55000|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C001|    O005|      P002|2024-01-10|Delivered|  2500|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|    O002|      P003|2024-01-05|Cancelled|     0|Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C003|    O003|      P004|2024-01-06|Delivered| 45000| Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C005|    O006|      P003|2024-01-12|Delivered|  1200| NULL|  35|  Chennai|                  []|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Join orders_df with broadcasted customers_df\n",
        "orders_customers_broadcast_df = orders_df.join(broadcast(customers_df), \"customer_id\", \"inner\")\n",
        "\n",
        "print(\"Physical plan for join with broadcasted customers_df:\")\n",
        "orders_customers_broadcast_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mnxxRXPTL7n",
        "outputId": "3bb3e009-dc5b-4a41-fb79-8e9eb461ce3b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical plan for join with broadcasted customers_df:\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [customer_id])\n",
            ":- Filter isnotnull(order_date#60)\n",
            ":  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            ":     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [customer_id#0, name#25, age#24, city#3, coalesce(interests#80, array_remove(split(, ,, -1), )) AS interests#82]\n",
            "      +- Project [customer_id#0, name#25, age#24, city#3, split(regexp_replace(interests#4, [|], ,, 1), ,, -1) AS interests#80]\n",
            "         +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN cast(null as string) ELSE name#1 END AS name#25, age#24, city#3, interests#4]\n",
            "            +- Project [customer_id#0, name#1, cast(CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#2, \\d+, 0) END as int) AS age#24, city#3, interests#4]\n",
            "               +- LogicalRDD [customer_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, order_id: string, product_id: string, order_date: date, status: string, amount: string, name: string, age: int, city: string, interests: array<string>\n",
            "Project [customer_id#15, order_id#14, product_id#16, order_date#60, status#18, amount#19, name#25, age#24, city#3, interests#82]\n",
            "+- Join Inner, (customer_id#15 = customer_id#0)\n",
            "   :- Filter isnotnull(order_date#60)\n",
            "   :  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            "   :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [customer_id#0, name#25, age#24, city#3, coalesce(interests#80, array_remove(split(, ,, -1), )) AS interests#82]\n",
            "         +- Project [customer_id#0, name#25, age#24, city#3, split(regexp_replace(interests#4, [|], ,, 1), ,, -1) AS interests#80]\n",
            "            +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN cast(null as string) ELSE name#1 END AS name#25, age#24, city#3, interests#4]\n",
            "               +- Project [customer_id#0, name#1, cast(CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#2, \\d+, 0) END as int) AS age#24, city#3, interests#4]\n",
            "                  +- LogicalRDD [customer_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [customer_id#15, order_id#14, product_id#16, order_date#60, status#18, amount#19, name#25, age#24, city#3, interests#82]\n",
            "+- Join Inner, (customer_id#15 = customer_id#0), rightHint=(strategy=broadcast)\n",
            "   :- Project [order_id#14, customer_id#15, product_id#16, coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#60, status#18, amount#19]\n",
            "   :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#15))\n",
            "   :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "   +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN null ELSE name#1 END AS name#25, CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#2, \\d+, 0) as int) END AS age#24, city#3, coalesce(split(regexp_replace(interests#4, [|], ,, 1), ,, -1), []) AS interests#82]\n",
            "      +- Filter isnotnull(customer_id#0)\n",
            "         +- LogicalRDD [customer_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#15, order_id#14, product_id#16, order_date#60, status#18, amount#19, name#25, age#24, city#3, interests#82]\n",
            "   +- BroadcastHashJoin [customer_id#15], [customer_id#0], Inner, BuildRight, false\n",
            "      :- Project [order_id#14, customer_id#15, product_id#16, coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#60, status#18, amount#19]\n",
            "      :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#15))\n",
            "      :     +- Scan ExistingRDD[order_id#14,customer_id#15,product_id#16,order_date#17,status#18,amount#19]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=470]\n",
            "         +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN null ELSE name#1 END AS name#25, CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#2, \\d+, 0) as int) END AS age#24, city#3, coalesce(split(regexp_replace(interests#4, [|], ,, 1), ,, -1), []) AS interests#82]\n",
            "            +- Filter isnotnull(customer_id#0)\n",
            "               +- Scan ExistingRDD[customer_id#0,name#1,age#2,city#3,interests#4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Join orders_df with broadcasted customers_df\n",
        "orders_customers_broadcast_df = orders_df.join(broadcast(customers_df), \"customer_id\", \"inner\")\n",
        "\n",
        "print(\"Physical plan for join with broadcasted customers_df:\")\n",
        "orders_customers_broadcast_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzBFeWJnTSd-",
        "outputId": "84990f84-5688-40cc-9539-0791131d6292"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical plan for join with broadcasted customers_df:\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [customer_id])\n",
            ":- Filter isnotnull(order_date#60)\n",
            ":  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            ":     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [customer_id#0, name#25, age#24, city#3, coalesce(interests#80, array_remove(split(, ,, -1), )) AS interests#82]\n",
            "      +- Project [customer_id#0, name#25, age#24, city#3, split(regexp_replace(interests#4, [|], ,, 1), ,, -1) AS interests#80]\n",
            "         +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN cast(null as string) ELSE name#1 END AS name#25, age#24, city#3, interests#4]\n",
            "            +- Project [customer_id#0, name#1, cast(CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#2, \\d+, 0) END as int) AS age#24, city#3, interests#4]\n",
            "               +- LogicalRDD [customer_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, order_id: string, product_id: string, order_date: date, status: string, amount: string, name: string, age: int, city: string, interests: array<string>\n",
            "Project [customer_id#15, order_id#14, product_id#16, order_date#60, status#18, amount#19, name#25, age#24, city#3, interests#82]\n",
            "+- Join Inner, (customer_id#15 = customer_id#0)\n",
            "   :- Filter isnotnull(order_date#60)\n",
            "   :  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            "   :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [customer_id#0, name#25, age#24, city#3, coalesce(interests#80, array_remove(split(, ,, -1), )) AS interests#82]\n",
            "         +- Project [customer_id#0, name#25, age#24, city#3, split(regexp_replace(interests#4, [|], ,, 1), ,, -1) AS interests#80]\n",
            "            +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN cast(null as string) ELSE name#1 END AS name#25, age#24, city#3, interests#4]\n",
            "               +- Project [customer_id#0, name#1, cast(CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#2, \\d+, 0) END as int) AS age#24, city#3, interests#4]\n",
            "                  +- LogicalRDD [customer_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [customer_id#15, order_id#14, product_id#16, order_date#60, status#18, amount#19, name#25, age#24, city#3, interests#82]\n",
            "+- Join Inner, (customer_id#15 = customer_id#0), rightHint=(strategy=broadcast)\n",
            "   :- Project [order_id#14, customer_id#15, product_id#16, coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#60, status#18, amount#19]\n",
            "   :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#15))\n",
            "   :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "   +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN null ELSE name#1 END AS name#25, CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#2, \\d+, 0) as int) END AS age#24, city#3, coalesce(split(regexp_replace(interests#4, [|], ,, 1), ,, -1), []) AS interests#82]\n",
            "      +- Filter isnotnull(customer_id#0)\n",
            "         +- LogicalRDD [customer_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#15, order_id#14, product_id#16, order_date#60, status#18, amount#19, name#25, age#24, city#3, interests#82]\n",
            "   +- BroadcastHashJoin [customer_id#15], [customer_id#0], Inner, BuildRight, false\n",
            "      :- Project [order_id#14, customer_id#15, product_id#16, coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#60, status#18, amount#19]\n",
            "      :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#15))\n",
            "      :     +- Scan ExistingRDD[order_id#14,customer_id#15,product_id#16,order_date#17,status#18,amount#19]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=505]\n",
            "         +- Project [customer_id#0, CASE WHEN (name#1 = ) THEN null ELSE name#1 END AS name#25, CASE WHEN (regexp_extract(age#2, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#2, \\d+, 0) as int) END AS age#24, city#3, coalesce(split(regexp_replace(interests#4, [|], ,, 1), ,, -1), []) AS interests#82]\n",
            "            +- Filter isnotnull(customer_id#0)\n",
            "               +- Scan ExistingRDD[customer_id#0,name#1,age#2,city#3,interests#4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11\n",
        "# 1. Eliminate customers without any orders\n",
        "customers_with_orders_df = customers_df.join(orders_df, \"customer_id\", \"left_semi\")\n",
        "orphan_customers_df = customers_df.join(orders_df, \"customer_id\", \"left_anti\")\n",
        "\n",
        "print(\"Orphan Customers:\")\n",
        "orphan_customers_df.show()\n",
        "\n",
        "# Update customers_df to only include customers with orders\n",
        "customers_df = customers_with_orders_df\n",
        "\n",
        "# 2. Eliminate products without any orders\n",
        "products_with_orders_df = products_df.join(orders_df, \"product_id\", \"left_semi\")\n",
        "orphan_products_df = products_df.join(orders_df, \"product_id\", \"left_anti\")\n",
        "\n",
        "print(\"Orphan Products:\")\n",
        "orphan_products_df.show()\n",
        "\n",
        "# Update products_df to only include products with orders\n",
        "products_df = products_with_orders_df\n",
        "\n",
        "# 3. Eliminate sellers without any products\n",
        "sellers_with_products_df = sellers_df.join(products_df, \"seller_id\", \"left_semi\")\n",
        "orphan_sellers_df = sellers_df.join(products_df, \"seller_id\", \"left_anti\")\n",
        "\n",
        "print(\"Orphan Sellers:\")\n",
        "orphan_sellers_df.show()\n",
        "\n",
        "# Update sellers_df to only include sellers with products\n",
        "sellers_df = sellers_with_products_df\n",
        "\n",
        "print(\"DataFrames after eliminating orphan records:\")\n",
        "customers_df.show()\n",
        "products_df.show()\n",
        "sellers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxrA4m5pTvcr",
        "outputId": "ceca0aff-8baa-41b3-963c-4602fa0102a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orphan Customers:\n",
            "+-----------+-------+---+----+--------------------+\n",
            "|customer_id|   name|age|city|           interests|\n",
            "+-----------+-------+---+----+--------------------+\n",
            "|       C004|Pallavi| 27|Pune|[Electronics, Bea...|\n",
            "+-----------+-------+---+----+--------------------+\n",
            "\n",
            "Orphan Products:\n",
            "+----------+----------+--------+---------+-----+\n",
            "|product_id|   product|category|seller_id|price|\n",
            "+----------+----------+--------+---------+-----+\n",
            "|      P005|Face Cream|  Beauty|     S004|  800|\n",
            "+----------+----------+--------+---------+-----+\n",
            "\n",
            "Orphan Sellers:\n",
            "+---------+-----------+--------+----------+\n",
            "|seller_id|seller_name|category|start_date|\n",
            "+---------+-----------+--------+----------+\n",
            "|     S004|BeautyStore|  Beauty|      NULL|\n",
            "+---------+-----------+--------+----------+\n",
            "\n",
            "DataFrames after eliminating orphan records:\n",
            "+-----------+-----+----+---------+--------------------+\n",
            "|customer_id| name| age|     city|           interests|\n",
            "+-----------+-----+----+---------+--------------------+\n",
            "|       C001|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C003| Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C005| NULL|  35|  Chennai|                  []|\n",
            "+-----------+-----+----+---------+--------------------+\n",
            "\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|product_id|   product|   category|seller_id|price|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|      P001|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|      Sofa|       Home|     S003|45000|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "\n",
            "+---------+--------------+-----------+----------+\n",
            "|seller_id|   seller_name|   category|start_date|\n",
            "+---------+--------------+-----------+----------+\n",
            "|     S001|     TechWorld|Electronics|2019-06-01|\n",
            "|     S002|    FashionHub|    Fashion|2020-07-01|\n",
            "|     S003|HomeEssentials|       Home|2018-09-15|\n",
            "+---------+--------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART C — ANALYTICS & AGGREGATIONS\n",
        "\n",
        "12. Total revenue per category\n",
        "13. Total revenue per seller\n",
        "14. Total orders per customer\n",
        "15. Average order value per customer\n",
        "16. Identify sellers with zero delivered orders"
      ],
      "metadata": {
        "id": "mEuGd8rvUvcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#12\n",
        "revenue_category_df = orders_products_df.groupBy(\"category\").agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "revenue_category_df.show()\n",
        "\n",
        "#13\n",
        "revenue_seller_df = orders_products_df.groupBy(\"seller_id\").agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "revenue_seller_df.show()\n",
        "\n",
        "#14\n",
        "orders_customers_df = orders_df.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"total_orders\"))\n",
        "orders_customers_df.show()\n",
        "\n",
        "#15\n",
        "average_order_value_df = orders_df.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
        "    .groupBy(\"customer_id\") \\\n",
        "    .agg(avg(\"amount\").alias(\"average_order_value\"))\n",
        "average_order_value_df.show()\n",
        "\n",
        "#16\n",
        "average_order_value_df = orders_df.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
        "    .groupBy(\"customer_id\") \\\n",
        "    .agg(avg(\"amount\").alias(\"average_order_value\"))\n",
        "average_order_value_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSEnaAJKT5LF",
        "outputId": "8556b2af-502e-4859-9de7-a580349178ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|       Home|      45000.0|\n",
            "|    Fashion|       1200.0|\n",
            "|Electronics|      57500.0|\n",
            "+-----------+-------------+\n",
            "\n",
            "+---------+-------------+\n",
            "|seller_id|total_revenue|\n",
            "+---------+-------------+\n",
            "|     S001|      57500.0|\n",
            "|     S002|       1200.0|\n",
            "|     S003|      45000.0|\n",
            "+---------+-------------+\n",
            "\n",
            "+-----------+------------+\n",
            "|customer_id|total_orders|\n",
            "+-----------+------------+\n",
            "|       C003|           1|\n",
            "|       C001|           2|\n",
            "|       C002|           1|\n",
            "|       C005|           1|\n",
            "+-----------+------------+\n",
            "\n",
            "+-----------+-------------------+\n",
            "|customer_id|average_order_value|\n",
            "+-----------+-------------------+\n",
            "|       C003|            45000.0|\n",
            "|       C001|            28750.0|\n",
            "|       C002|                0.0|\n",
            "|       C005|             1200.0|\n",
            "+-----------+-------------------+\n",
            "\n",
            "+-----------+-------------------+\n",
            "|customer_id|average_order_value|\n",
            "+-----------+-------------------+\n",
            "|       C003|            45000.0|\n",
            "|       C001|            28750.0|\n",
            "|       C002|                0.0|\n",
            "|       C005|             1200.0|\n",
            "+-----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART D — WINDOW FUNCTIONS\n",
        "\n",
        "17. Rank customers by total spend (overall)\n",
        "18. Rank sellers by revenue within each category\n",
        "19. Calculate running revenue per day\n",
        "20. Identify top 2 products per category by revenue"
      ],
      "metadata": {
        "id": "mffgsd50VMNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#17\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, rank\n",
        "\n",
        "# Calculate total spend per customer\n",
        "total_spend_per_customer_df = orders_products_df.groupBy(\"customer_id\") \\\n",
        "    .agg(sum(col(\"amount\").cast(\"double\")).alias(\"total_spend\"))\n",
        "\n",
        "# Define a window specification to rank customers by total spend\n",
        "window_spec = Window.orderBy(col(\"total_spend\").desc())\n",
        "\n",
        "# Apply the rank function\n",
        "customer_spend_rank_df = total_spend_per_customer_df.withColumn(\"spend_rank\", rank().over(window_spec))\n",
        "\n",
        "customer_spend_rank_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPr9qfAIVLlq",
        "outputId": "47b6610a-3fa7-4c9e-cf25-e7fdbb5bbef2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+----------+\n",
            "|customer_id|total_spend|spend_rank|\n",
            "+-----------+-----------+----------+\n",
            "|       C001|    57500.0|         1|\n",
            "|       C003|    45000.0|         2|\n",
            "|       C005|     1200.0|         3|\n",
            "|       C002|        0.0|         4|\n",
            "+-----------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, rank\n",
        "\n",
        "revenue_per_seller_category_df = orders_products_df.groupBy(\"category\", \"seller_id\") \\\n",
        "    .agg(sum(col(\"amount\").cast(\"double\")).alias(\"total_revenue\"))\n",
        "\n",
        "window_spec_category = Window.partitionBy(\"category\").orderBy(col(\"total_revenue\").desc())\n",
        "\n",
        "seller_category_rank_df = revenue_per_seller_category_df.withColumn(\"category_rank\", rank().over(window_spec_category))\n",
        "\n",
        "seller_category_rank_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2016ne70VS_0",
        "outputId": "865d3156-8923-4a83-a03f-98816ed15e3f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------------+-------------+\n",
            "|   category|seller_id|total_revenue|category_rank|\n",
            "+-----------+---------+-------------+-------------+\n",
            "|Electronics|     S001|      57500.0|            1|\n",
            "|    Fashion|     S002|       1200.0|            1|\n",
            "|       Home|     S003|      45000.0|            1|\n",
            "+-----------+---------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, asc\n",
        "\n",
        "daily_revenue_df = orders_products_df.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
        "    .groupBy(\"order_date\") \\\n",
        "    .agg(sum(\"amount\").alias(\"daily_revenue\"))\n",
        "\n",
        "\n",
        "window_spec_daily = Window.orderBy(asc(\"order_date\"))\n",
        "\n",
        "running_revenue_df = daily_revenue_df.withColumn(\"running_revenue\", sum(\"daily_revenue\").over(window_spec_daily))\n",
        "\n",
        "running_revenue_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBF3a5FiVXzU",
        "outputId": "ba0bb6d1-fb48-4553-ca3a-a38697ecefbd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------------+\n",
            "|order_date|daily_revenue|running_revenue|\n",
            "+----------+-------------+---------------+\n",
            "|2024-01-05|      55000.0|        55000.0|\n",
            "|2024-01-06|      45000.0|       100000.0|\n",
            "|2024-01-10|       2500.0|       102500.0|\n",
            "|2024-01-12|       1200.0|       103700.0|\n",
            "+----------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, rank\n",
        "\n",
        "product_revenue_per_category_df = orders_products_df.groupBy(\"category\", \"product_id\", \"product\") \\\n",
        "    .agg(sum(col(\"amount\").cast(\"double\")).alias(\"product_revenue\"))\n",
        "\n",
        "window_spec_product_rank = Window.partitionBy(\"category\").orderBy(col(\"product_revenue\").desc())\n",
        "\n",
        "top_2_products_per_category_df = product_revenue_per_category_df.withColumn(\"rank\", rank().over(window_spec_product_rank)) \\\n",
        "    .filter(col(\"rank\") <= 2)\n",
        "\n",
        "top_2_products_per_category_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaHgl6SWVhKa",
        "outputId": "39c27eb5-b7af-4f46-9aa0-c77fb05bf4d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+----------+---------------+----+\n",
            "|   category|product_id|   product|product_revenue|rank|\n",
            "+-----------+----------+----------+---------------+----+\n",
            "|Electronics|      P001|    Laptop|        55000.0|   1|\n",
            "|Electronics|      P002|Headphones|         2500.0|   2|\n",
            "|    Fashion|      P003|   T-Shirt|         1200.0|   1|\n",
            "|       Home|      P004|      Sofa|        45000.0|   1|\n",
            "+-----------+----------+----------+---------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART E — UDF (ONLY IF REQUIRED)\n",
        "\n",
        "21. Classify customers into spending tiers:\n",
        "\n",
        "- High\n",
        "- Medium\n",
        "- Low\n",
        "\n",
        "Rules:\n",
        "\n",
        "- Prefer built-in functions\n",
        "- Use UDF only if unavoidable\n",
        "- Justify your choice"
      ],
      "metadata": {
        "id": "NNmqiggAV5xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "customer_spending_tiers_df = total_spend_per_customer_df.withColumn(\n",
        "    \"spending_tier\",\n",
        "    when(col(\"total_spend\") > 10000, \"High\")\n",
        "    .when((col(\"total_spend\") > 1000) & (col(\"total_spend\") <= 10000), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "print(\"Customers classified into spending tiers:\")\n",
        "customer_spending_tiers_df.show()\n",
        "\n",
        "# Justification for not using a UDF:\n",
        "# PySpark's `when().otherwise()` provides native, optimized functionality for conditional logic.\n",
        "# It is executed within the Spark engine, benefiting from Catalyst Optimizer and Tungsten execution engine,\n",
        "# leading to significantly better performance compared to Python UDFs. UDFs involve serialization/deserialization\n",
        "# overhead and context switching between JVM and Python, which can be very slow for large datasets.\n",
        "# Since `when().otherwise()` perfectly handles the tier classification logic, a UDF is unnecessary and less efficient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bjf9yqV4_z",
        "outputId": "67dcb823-0ce3-4b40-8703-e0b9e3cc29b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers classified into spending tiers:\n",
            "+-----------+-----------+-------------+\n",
            "|customer_id|total_spend|spending_tier|\n",
            "+-----------+-----------+-------------+\n",
            "|       C003|    45000.0|         High|\n",
            "|       C005|     1200.0|       Medium|\n",
            "|       C001|    57500.0|         High|\n",
            "|       C002|        0.0|          Low|\n",
            "+-----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART F — SORTING & ORDERING\n",
        "\n",
        "22. Sort categories by total revenue (descending)\n",
        "23. Sort sellers by revenue within category\n",
        "24. Explain why sorting caused a shuffle"
      ],
      "metadata": {
        "id": "z_MYizjQXns7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#22\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "sorted_categories_by_revenue_df = revenue_category_df.orderBy(desc(\"total_revenue\"))\n",
        "sorted_categories_by_revenue_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKS0lCZwXnK8",
        "outputId": "c7413bd8-110f-4923-d397-04205d8bdb5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|Electronics|      57500.0|\n",
            "|       Home|      45000.0|\n",
            "|    Fashion|       1200.0|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23\n",
        "from pyspark.sql.functions import col\n",
        "sorted_sellers_by_category_revenue_df = seller_category_rank_df.orderBy(col(\"category\").asc(), col(\"total_revenue\").desc())\n",
        "\n",
        "print(\"Sellers sorted by revenue within each category:\")\n",
        "sorted_sellers_by_category_revenue_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDXq_zJrYD-8",
        "outputId": "44525cb4-155a-49a3-f88f-6dee23d5079f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sellers sorted by revenue within each category:\n",
            "+-----------+---------+-------------+-------------+\n",
            "|   category|seller_id|total_revenue|category_rank|\n",
            "+-----------+---------+-------------+-------------+\n",
            "|Electronics|     S001|      57500.0|            1|\n",
            "|    Fashion|     S002|       1200.0|            1|\n",
            "|       Home|     S003|      45000.0|            1|\n",
            "+-----------+---------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting a DataFrame in Spark often triggers a 'shuffle' operation.\n",
        "# A shuffle is the process of redistributing data across partitions (and potentially across machines in a cluster).\n",
        "# This is necessary because to perform a global sort (or even a sort within groups if data is not pre-partitioned\n",
        "# or pre-sorted), all data relevant to a specific sort key range might need to be collected on the same partition.\n",
        "# For example, when sorting categories by total revenue, Spark needs to know the total revenue for all categories\n",
        "# to correctly order them. If different parts of a category's data reside on different partitions,\n",
        "# Spark must move this data to ensure a consistent global order. This involves serializing data,\n",
        "# sending it over the network, and deserializing it on the receiving end, which is a resource-intensive operation."
      ],
      "metadata": {
        "id": "csJ6rPgDYwes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART G — SET OPERATIONS\n",
        "\n",
        "Create two DataFrames:\n",
        "- Customers who placed orders\n",
        "- Customers who were active (search/view)\n",
        "25. Find customers who were active but never ordered\n",
        "26. Find customers who ordered and were active\n",
        "27. Explain why set operations differ from joins"
      ],
      "metadata": {
        "id": "qDy_nDc1ZG4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_customers_df = orders_df.select(\"customer_id\").distinct()\n",
        "active_customers_df = activity_df.filter(col(\"actions\").isNotNull()).select(\"customer_id\").distinct()"
      ],
      "metadata": {
        "id": "Z6sOobcLZO5A"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25\n",
        "active_customers_df.subtract(ordered_customers_df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unFsFtQ_ZagY",
        "outputId": "634def75-79b5-4f8c-e97c-124b904c8109"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#26\n",
        "active_customers_df.intersect(ordered_customers_df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDcXlkSBZlAZ",
        "outputId": "d0ece403-89f5-416f-b5a6-46d9334b5ddc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|       C003|\n",
            "|       C005|\n",
            "|       C001|\n",
            "|       C002|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#27\n",
        "# Differences between Set Operations and Joins:\n",
        "#\n",
        "# Set Operations (UNION, INTERSECT, EXCEPT/SUBTRACT):\n",
        "# - Operate on the *rows* of DataFrames.\n",
        "# - Require the DataFrames to have a compatible schema (same number of columns, same column names, and compatible data types).\n",
        "# - Combine or compare rows based on their *entire content*.\n",
        "# - The result has the same schema as the input DataFrames.\n",
        "#\n",
        "# Join Operations (INNER, LEFT, RIGHT, FULL, ANTI, SEMI):\n",
        "# - Combine *columns* from two DataFrames.\n",
        "# - Combine data based on a *common key* or a specified condition.\n",
        "# - Typically result in a wider DataFrame (more columns) by merging information from both DataFrames.\n",
        "# - The schema of the result is a combination of the schemas of the input DataFrames (excluding duplicate join keys if specified).\n",
        "\n",
        "print(\"\\n--- Set Operations (Operating on rows) ---\")\n",
        "print(\"Customers active but never ordered (using subtract):\")\n",
        "active_customers_df.subtract(ordered_customers_df).show()\n",
        "\n",
        "print(\"Customers who ordered AND were active (using intersect):\")\n",
        "active_customers_df.intersect(ordered_customers_df).show()\n",
        "\n",
        "print(\"\\n--- Join Operations (Operating on columns based on keys) ---\")\n",
        "print(\"Inner Join: Combining customer and order details for matching customer_ids:\")\n",
        "orders_df.join(customers_df, \"customer_id\", \"inner\").show()\n",
        "\n",
        "print(\"Left Anti Join: Customers from 'customers_df' who are NOT in 'orders_df' (different from subtract, key-based):\")\n",
        "customers_df.join(orders_df, \"customer_id\", \"left_anti\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOI7AbX6ZoTw",
        "outputId": "d932ee71-9278-455c-ad84-d9a32176f901"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Set Operations (Operating on rows) ---\n",
            "Customers active but never ordered (using subtract):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "+-----------+\n",
            "\n",
            "Customers who ordered AND were active (using intersect):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|       C003|\n",
            "|       C005|\n",
            "|       C001|\n",
            "|       C002|\n",
            "+-----------+\n",
            "\n",
            "\n",
            "--- Join Operations (Operating on columns based on keys) ---\n",
            "Inner Join: Combining customer and order details for matching customer_ids:\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|customer_id|order_id|product_id|order_date|   status|amount| name| age|     city|           interests|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|       C003|    O003|      P004|2024-01-06|Delivered| 45000| Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C001|    O001|      P001|2024-01-05|Delivered| 55000|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|    O002|      P003|2024-01-05|Cancelled|     0|Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C005|    O006|      P003|2024-01-12|Delivered|  1200| NULL|  35|  Chennai|                  []|\n",
            "|       C001|    O005|      P002|2024-01-10|Delivered|  2500|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "\n",
            "Left Anti Join: Customers from 'customers_df' who are NOT in 'orders_df' (different from subtract, key-based):\n",
            "+-----------+----+---+----+---------+\n",
            "|customer_id|name|age|city|interests|\n",
            "+-----------+----+---+----+---------+\n",
            "+-----------+----+---+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART H — DAG & PERFORMANCE ANALYSIS\n",
        "\n",
        "28. Run explain(True) for:\n",
        "- Product → Seller join\n",
        "- Window ranking\n",
        "- Sorting\n",
        "29. Identify:\n",
        "- Shuffles\n",
        "- Broadcast joins\n",
        "Sort stages\n",
        "30. Suggest one performance improvement"
      ],
      "metadata": {
        "id": "J7-SLBmeZ4bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#28\n",
        "products_seller_df.explain(True)\n",
        "customer_spend_rank_df.explain(True)\n",
        "sorted_categories_by_revenue_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EEth4eZZ4D4",
        "outputId": "bc388deb-edab-4c22-ec7f-a9399b97226b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [seller_id])\n",
            ":- Project [product_id#9, product#10, category#11, seller_id#12, cast(price#42 as int) AS price#43]\n",
            ":  +- Project [product_id#9, product#10, category#11, seller_id#12, start_date#13 AS price#42]\n",
            ":     +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [seller_id#5, seller_name#6, category#7, coalesce(to_date(try_to_timestamp(start_date#8, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#8, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#8, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS start_date#83]\n",
            "      +- LogicalRDD [seller_id#5, seller_name#6, category#7, start_date#8], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "seller_id: string, product_id: string, product: string, category: string, price: int, seller_name: string, category: string, start_date: date\n",
            "Project [seller_id#12, product_id#9, product#10, category#11, price#43, seller_name#6, category#7, start_date#83]\n",
            "+- Join Inner, (seller_id#12 = seller_id#5)\n",
            "   :- Project [product_id#9, product#10, category#11, seller_id#12, cast(price#42 as int) AS price#43]\n",
            "   :  +- Project [product_id#9, product#10, category#11, seller_id#12, start_date#13 AS price#42]\n",
            "   :     +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [seller_id#5, seller_name#6, category#7, coalesce(to_date(try_to_timestamp(start_date#8, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#8, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#8, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS start_date#83]\n",
            "         +- LogicalRDD [seller_id#5, seller_name#6, category#7, start_date#8], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [seller_id#12, product_id#9, product#10, category#11, price#43, seller_name#6, category#7, start_date#83]\n",
            "+- Join Inner, (seller_id#12 = seller_id#5), rightHint=(strategy=broadcast)\n",
            "   :- Project [product_id#9, product#10, category#11, seller_id#12, cast(start_date#13 as int) AS price#43]\n",
            "   :  +- Filter isnotnull(seller_id#12)\n",
            "   :     +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "   +- Project [seller_id#5, seller_name#6, category#7, coalesce(cast(gettimestamp(start_date#8, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#8, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#8, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS start_date#83]\n",
            "      +- Filter isnotnull(seller_id#5)\n",
            "         +- LogicalRDD [seller_id#5, seller_name#6, category#7, start_date#8], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [seller_id#12, product_id#9, product#10, category#11, price#43, seller_name#6, category#7, start_date#83]\n",
            "   +- BroadcastHashJoin [seller_id#12], [seller_id#5], Inner, BuildRight, false\n",
            "      :- Project [product_id#9, product#10, category#11, seller_id#12, cast(start_date#13 as int) AS price#43]\n",
            "      :  +- Filter isnotnull(seller_id#12)\n",
            "      :     +- Scan ExistingRDD[product_id#9,product#10,category#11,seller_id#12,start_date#13]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=5887]\n",
            "         +- Project [seller_id#5, seller_name#6, category#7, coalesce(cast(gettimestamp(start_date#8, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#8, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#8, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS start_date#83]\n",
            "            +- Filter isnotnull(seller_id#5)\n",
            "               +- Scan ExistingRDD[seller_id#5,seller_name#6,category#7,start_date#8]\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(spend_rank, 'rank() windowspecdefinition('total_spend DESC NULLS LAST, unspecifiedframe$()), None)]\n",
            "+- Aggregate [customer_id#15], [customer_id#15, sum(cast(amount#19 as double)) AS total_spend#448]\n",
            "   +- Project [product_id#16, order_id#14, customer_id#15, order_date#60, status#18, amount#19, product#10, category#11, seller_id#12, price#43]\n",
            "      +- Join Inner, (product_id#16 = product_id#9)\n",
            "         :- Filter isnotnull(order_date#60)\n",
            "         :  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            "         :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "         +- Project [product_id#9, product#10, category#11, seller_id#12, cast(price#42 as int) AS price#43]\n",
            "            +- Project [product_id#9, product#10, category#11, seller_id#12, start_date#13 AS price#42]\n",
            "               +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, total_spend: double, spend_rank: int\n",
            "Project [customer_id#15, total_spend#448, spend_rank#460]\n",
            "+- Project [customer_id#15, total_spend#448, spend_rank#460, spend_rank#460]\n",
            "   +- Window [rank(total_spend#448) windowspecdefinition(total_spend#448 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS spend_rank#460], [total_spend#448 DESC NULLS LAST]\n",
            "      +- Project [customer_id#15, total_spend#448]\n",
            "         +- Aggregate [customer_id#15], [customer_id#15, sum(cast(amount#19 as double)) AS total_spend#448]\n",
            "            +- Project [product_id#16, order_id#14, customer_id#15, order_date#60, status#18, amount#19, product#10, category#11, seller_id#12, price#43]\n",
            "               +- Join Inner, (product_id#16 = product_id#9)\n",
            "                  :- Filter isnotnull(order_date#60)\n",
            "                  :  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            "                  :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "                  +- Project [product_id#9, product#10, category#11, seller_id#12, cast(price#42 as int) AS price#43]\n",
            "                     +- Project [product_id#9, product#10, category#11, seller_id#12, start_date#13 AS price#42]\n",
            "                        +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [rank(total_spend#448) windowspecdefinition(total_spend#448 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS spend_rank#460], [total_spend#448 DESC NULLS LAST]\n",
            "+- Aggregate [customer_id#15], [customer_id#15, sum(cast(amount#19 as double)) AS total_spend#448]\n",
            "   +- Project [customer_id#15, amount#19]\n",
            "      +- Join Inner, (product_id#16 = product_id#9)\n",
            "         :- Project [customer_id#15, product_id#16, amount#19]\n",
            "         :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#16))\n",
            "         :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "         +- Project [product_id#9]\n",
            "            +- Filter isnotnull(product_id#9)\n",
            "               +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Window [rank(total_spend#448) windowspecdefinition(total_spend#448 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS spend_rank#460], [total_spend#448 DESC NULLS LAST]\n",
            "   +- Sort [total_spend#448 DESC NULLS LAST], false, 0\n",
            "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=5945]\n",
            "         +- HashAggregate(keys=[customer_id#15], functions=[sum(cast(amount#19 as double))], output=[customer_id#15, total_spend#448])\n",
            "            +- Exchange hashpartitioning(customer_id#15, 200), ENSURE_REQUIREMENTS, [plan_id=5942]\n",
            "               +- HashAggregate(keys=[customer_id#15], functions=[partial_sum(cast(amount#19 as double))], output=[customer_id#15, sum#473])\n",
            "                  +- Project [customer_id#15, amount#19]\n",
            "                     +- SortMergeJoin [product_id#16], [product_id#9], Inner\n",
            "                        :- Sort [product_id#16 ASC NULLS FIRST], false, 0\n",
            "                        :  +- Exchange hashpartitioning(product_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=5934]\n",
            "                        :     +- Project [customer_id#15, product_id#16, amount#19]\n",
            "                        :        +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#16))\n",
            "                        :           +- Scan ExistingRDD[order_id#14,customer_id#15,product_id#16,order_date#17,status#18,amount#19]\n",
            "                        +- Sort [product_id#9 ASC NULLS FIRST], false, 0\n",
            "                           +- Exchange hashpartitioning(product_id#9, 200), ENSURE_REQUIREMENTS, [plan_id=5935]\n",
            "                              +- Project [product_id#9]\n",
            "                                 +- Filter isnotnull(product_id#9)\n",
            "                                    +- Scan ExistingRDD[product_id#9,product#10,category#11,seller_id#12,start_date#13]\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Sort ['total_revenue DESC NULLS LAST], true\n",
            "+- Aggregate [category#11], [category#11, sum(cast(amount#19 as double)) AS total_revenue#338]\n",
            "   +- Project [product_id#16, order_id#14, customer_id#15, order_date#60, status#18, amount#19, product#10, category#11, seller_id#12, price#43]\n",
            "      +- Join Inner, (product_id#16 = product_id#9)\n",
            "         :- Filter isnotnull(order_date#60)\n",
            "         :  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            "         :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "         +- Project [product_id#9, product#10, category#11, seller_id#12, cast(price#42 as int) AS price#43]\n",
            "            +- Project [product_id#9, product#10, category#11, seller_id#12, start_date#13 AS price#42]\n",
            "               +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "category: string, total_revenue: double\n",
            "Sort [total_revenue#338 DESC NULLS LAST], true\n",
            "+- Aggregate [category#11], [category#11, sum(cast(amount#19 as double)) AS total_revenue#338]\n",
            "   +- Project [product_id#16, order_id#14, customer_id#15, order_date#60, status#18, amount#19, product#10, category#11, seller_id#12, price#43]\n",
            "      +- Join Inner, (product_id#16 = product_id#9)\n",
            "         :- Filter isnotnull(order_date#60)\n",
            "         :  +- Project [order_id#14, customer_id#15, product_id#16, coalesce(to_date(try_to_timestamp(order_date#17, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#17, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#60, status#18, amount#19]\n",
            "         :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "         +- Project [product_id#9, product#10, category#11, seller_id#12, cast(price#42 as int) AS price#43]\n",
            "            +- Project [product_id#9, product#10, category#11, seller_id#12, start_date#13 AS price#42]\n",
            "               +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [total_revenue#338 DESC NULLS LAST], true\n",
            "+- Aggregate [category#11], [category#11, sum(cast(amount#19 as double)) AS total_revenue#338]\n",
            "   +- Project [amount#19, category#11]\n",
            "      +- Join Inner, (product_id#16 = product_id#9)\n",
            "         :- Project [product_id#16, amount#19]\n",
            "         :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#16))\n",
            "         :     +- LogicalRDD [order_id#14, customer_id#15, product_id#16, order_date#17, status#18, amount#19], false\n",
            "         +- Project [product_id#9, category#11]\n",
            "            +- Filter isnotnull(product_id#9)\n",
            "               +- LogicalRDD [product_id#9, product#10, category#11, seller_id#12, start_date#13], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [total_revenue#338 DESC NULLS LAST], true, 0\n",
            "   +- Exchange rangepartitioning(total_revenue#338 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=6003]\n",
            "      +- HashAggregate(keys=[category#11], functions=[sum(cast(amount#19 as double))], output=[category#11, total_revenue#338])\n",
            "         +- Exchange hashpartitioning(category#11, 200), ENSURE_REQUIREMENTS, [plan_id=6000]\n",
            "            +- HashAggregate(keys=[category#11], functions=[partial_sum(cast(amount#19 as double))], output=[category#11, sum#353])\n",
            "               +- Project [amount#19, category#11]\n",
            "                  +- SortMergeJoin [product_id#16], [product_id#9], Inner\n",
            "                     :- Sort [product_id#16 ASC NULLS FIRST], false, 0\n",
            "                     :  +- Exchange hashpartitioning(product_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=5992]\n",
            "                     :     +- Project [product_id#16, amount#19]\n",
            "                     :        +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#17, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#17, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#16))\n",
            "                     :           +- Scan ExistingRDD[order_id#14,customer_id#15,product_id#16,order_date#17,status#18,amount#19]\n",
            "                     +- Sort [product_id#9 ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(product_id#9, 200), ENSURE_REQUIREMENTS, [plan_id=5993]\n",
            "                           +- Project [product_id#9, category#11]\n",
            "                              +- Filter isnotnull(product_id#9)\n",
            "                                 +- Scan ExistingRDD[product_id#9,product#10,category#11,seller_id#12,start_date#13]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#29\n",
        "#Shuffles: GroupBy, sort\n",
        "#Broadcast joins: Seller join\n",
        "#Sort stages: Window+OrderBy"
      ],
      "metadata": {
        "id": "W5-tny_BaEsM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30\n",
        "# Performance Improvement Suggestion:\n",
        "\n",
        "orders_products_df.cache()\n",
        "# Cache the 'orders_products_df' DataFrame.\n",
        "# This DataFrame is the result of a join and is used multiple times in subsequent calculations\n",
        "# (e.g., total revenue per category/seller, running revenue, top products).\n",
        "# Caching it will prevent Spark from recomputing this DataFrame every time it's accessed."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IbtqB-ZaLsw",
        "outputId": "2273e3db-27ff-4ab9-d773-8c516b9f3069"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product_id: string, order_id: string, customer_id: string, order_date: date, status: string, amount: string, product: string, category: string, seller_id: string, price: int]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}